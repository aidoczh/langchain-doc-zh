# å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ç±»
:::info å…ˆå†³æ¡ä»¶
æœ¬æŒ‡å—å‡å®šæ‚¨ç†Ÿæ‚‰ä»¥ä¸‹æ¦‚å¿µï¼š
- [èŠå¤©æ¨¡å‹](/docs/concepts/#chat-models)
:::
åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ LangChain æŠ½è±¡åˆ›å»ºè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ã€‚
é€šè¿‡å°†æ‚¨çš„ LLM ä¸æ ‡å‡†çš„ [`BaseChatModel`](https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) æ¥å£åŒ…è£…ï¼Œæ‚¨å¯ä»¥åœ¨ç°æœ‰çš„ LangChain ç¨‹åºä¸­ä½¿ç”¨æ‚¨çš„ LLMï¼Œå¹¶ä¸”åªéœ€è¿›è¡Œæœ€å°‘çš„ä»£ç ä¿®æ”¹ï¼
ä½œä¸ºé¢å¤–ç¦åˆ©ï¼Œæ‚¨çš„ LLM å°†è‡ªåŠ¨æˆä¸º LangChain çš„ `Runnable`ï¼Œå¹¶å°†ä»ä¸€äº›ä¼˜åŒ–ä¸­å—ç›Šï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡çº¿ç¨‹æ± è¿›è¡Œæ‰¹å¤„ç†ï¼‰ï¼Œå¼‚æ­¥æ”¯æŒï¼Œ`astream_events` API ç­‰ã€‚
## è¾“å…¥å’Œè¾“å‡º
é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¨è®º**æ¶ˆæ¯**ï¼Œè¿™äº›æ˜¯èŠå¤©æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºã€‚
### æ¶ˆæ¯
èŠå¤©æ¨¡å‹å°†æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ¶ˆæ¯ä½œä¸ºè¾“å‡ºã€‚
LangChain æœ‰ä¸€äº›[å†…ç½®æ¶ˆæ¯ç±»å‹](/docs/concepts/#message-types)ï¼š
| æ¶ˆæ¯ç±»å‹              | æè¿°                                                                                           |
|-----------------------|-----------------------------------------------------------------------------------------------|
| `SystemMessage`       | ç”¨äºå¯åŠ¨ AI è¡Œä¸ºï¼Œé€šå¸¸ä½œä¸ºè¾“å…¥æ¶ˆæ¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ¶ˆæ¯ä¼ é€’ã€‚                                       |
| `HumanMessage`        | è¡¨ç¤ºä¸èŠå¤©æ¨¡å‹äº¤äº’çš„äººçš„æ¶ˆæ¯ã€‚                                                                 |
| `AIMessage`           | è¡¨ç¤ºæ¥è‡ªèŠå¤©æ¨¡å‹çš„æ¶ˆæ¯ã€‚è¿™å¯ä»¥æ˜¯æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥æ˜¯è¯·æ±‚è°ƒç”¨å·¥å…·ã€‚                                  |
| `FunctionMessage` / `ToolMessage` | ç”¨äºå°†å·¥å…·è°ƒç”¨çš„ç»“æœä¼ é€’å›æ¨¡å‹çš„æ¶ˆæ¯ã€‚                                       |
| `AIMessageChunk` / `HumanMessageChunk` / ... | æ¯ç§æ¶ˆæ¯ç±»å‹çš„åˆ†å—å˜ä½“ã€‚ |
::: {.callout-note}
`ToolMessage` å’Œ `FunctionMessage` ç´§éš OpenAI çš„ `function` å’Œ `tool` è§’è‰²ã€‚
è¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œéšç€æ›´å¤šæ¨¡å‹æ·»åŠ å‡½æ•°è°ƒç”¨åŠŸèƒ½ï¼Œé¢„è®¡å°†ä¼šå¯¹æ­¤æ¨¡å¼è¿›è¡Œè¡¥å……ã€‚
:::
```python
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    FunctionMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
```
### æµå¼å˜ä½“
æ‰€æœ‰èŠå¤©æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªåŒ…å« `Chunk` çš„æµå¼å˜ä½“ã€‚
```python
from langchain_core.messages import (
    AIMessageChunk,
    FunctionMessageChunk,
    HumanMessageChunk,
    SystemMessageChunk,
    ToolMessageChunk,
)
```
è¿™äº›åˆ†å—åœ¨ä»èŠå¤©æ¨¡å‹ä¸­è¾“å‡ºæµæ—¶ä½¿ç”¨ï¼Œå¹¶ä¸”å®ƒä»¬éƒ½å®šä¹‰äº†ä¸€ä¸ªå¯æ·»åŠ çš„å±æ€§ï¼
```python
AIMessageChunk(content="Hello") + AIMessageChunk(content=" World!")
```
```output
AIMessageChunk(content='Hello World!')
```
## åŸºç¡€èŠå¤©æ¨¡å‹
è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªèŠå¤©æ¨¡å‹ï¼Œå®ƒä¼šå›æ˜¾æç¤ºä¸­æœ€åä¸€æ¡æ¶ˆæ¯çš„å‰ `n` ä¸ªå­—ç¬¦ï¼
ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ç»§æ‰¿è‡ª `BaseChatModel`ï¼Œå¹¶ä¸”æˆ‘ä»¬éœ€è¦å®ç°ä»¥ä¸‹å†…å®¹ï¼š
| æ–¹æ³•/å±æ€§                    | æè¿°                                                       | å¿…éœ€/å¯é€‰  |
|------------------------------------|-------------------------------------------------------------------|--------------------|
| `_generate`                        | ç”¨äºä»æç¤ºç”ŸæˆèŠå¤©ç»“æœ                       | å¿…éœ€           |
| `_llm_type` (å±æ€§)             | ç”¨äºå”¯ä¸€æ ‡è¯†æ¨¡å‹çš„ç±»å‹ã€‚ç”¨äºæ—¥å¿—è®°å½•ã€‚| å¿…éœ€           |
| `_identifying_params` (å±æ€§)   | ç”¨äºè·Ÿè¸ªç›®çš„è¡¨ç¤ºæ¨¡å‹å‚æ•°åŒ–ã€‚            | å¯é€‰           |
| `_stream`                          | ç”¨äºå®ç°æµå¼å¤„ç†ã€‚                                       | å¯é€‰           |
| `_agenerate`                       | ç”¨äºå®ç°æœ¬æœºå¼‚æ­¥æ–¹æ³•ã€‚                           | å¯é€‰           |
| `_astream`                         | ç”¨äºå®ç° `_stream` çš„å¼‚æ­¥ç‰ˆæœ¬ã€‚                      | å¯é€‰           |
:::tip
`_astream` å®ç°ä½¿ç”¨ `run_in_executor` åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­å¯åŠ¨åŒæ­¥ `_stream`ï¼Œå¦‚æœå®ç°äº† `_stream`ï¼Œå¦åˆ™å°†å›é€€åˆ°ä½¿ç”¨ `_agenerate`ã€‚
å¦‚æœæ‚¨æƒ³è¦é‡ç”¨ `_stream` å®ç°ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªæŠ€å·§ï¼Œä½†å¦‚æœèƒ½å¤Ÿå®ç°åŸç”Ÿå¼‚æ­¥ä»£ç ï¼Œé‚£å°†æ˜¯æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºè¯¥ä»£ç å°†ä»¥æ›´å°‘çš„å¼€é”€è¿è¡Œã€‚
:::
### å®ç°
```python
from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.language_models import BaseChatModel, SimpleChatModel
from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage
from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
from langchain_core.runnables import run_in_executor
class CustomChatModelAdvanced(BaseChatModel):
    """ä¸€ä¸ªè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ï¼Œå›æ˜¾æç¤ºä¸­æœ€åä¸€æ¡æ¶ˆæ¯çš„å‰ `n` ä¸ªå­—ç¬¦ã€‚
    åœ¨ä¸º LangChain è´¡çŒ®å®ç°æ—¶ï¼Œä»”ç»†è®°å½•
    åŒ…æ‹¬æ¨¡å‹çš„åˆå§‹åŒ–å‚æ•°ï¼ŒåŒ…æ‹¬
    å¦‚ä½•åˆå§‹åŒ–æ¨¡å‹çš„ç¤ºä¾‹ï¼Œå¹¶åŒ…æ‹¬ä»»ä½•ç›¸å…³çš„
    é“¾æ¥åˆ°åº•å±‚æ¨¡å‹æ–‡æ¡£æˆ– APIã€‚
    ç¤ºä¾‹ï¼š
        .. code-block:: python
            model = CustomChatModel(n=2)
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """
    model_name: str
    """æ¨¡å‹çš„åç§°"""
    n: int
    """è¦å›æ˜¾çš„æç¤ºä¸­æœ€åä¸€æ¡æ¶ˆæ¯çš„å­—ç¬¦æ•°ã€‚"""
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """è¦†ç›– _generate æ–¹æ³•ä»¥å®ç°èŠå¤©æ¨¡å‹é€»è¾‘ã€‚
        è¿™å¯ä»¥æ˜¯å¯¹ API çš„è°ƒç”¨ï¼Œå¯¹æœ¬åœ°æ¨¡å‹çš„è°ƒç”¨ï¼Œæˆ–è€…ç”Ÿæˆå¯¹è¾“å…¥æç¤ºçš„å“åº”çš„ä»»ä½•å…¶ä»–
        å®ç°ã€‚
        Args:
            messages: ç”±æ¶ˆæ¯åˆ—è¡¨ç»„æˆçš„æç¤ºã€‚
            stop: æ¨¡å‹åº”åœæ­¢ç”Ÿæˆçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
                  å¦‚æœç”Ÿæˆç”±äºåœæ­¢ä»¤ç‰Œè€Œåœæ­¢ï¼Œåˆ™åœæ­¢ä»¤ç‰Œæœ¬èº«
                  åº”åŒ…å«åœ¨è¾“å‡ºä¸­ã€‚ç›®å‰å°šæœªå¼ºåˆ¶æ‰§è¡Œ
                  è·¨æ¨¡å‹ï¼Œä½†éµå¾ªè¿™ä¸€å®è·µæ˜¯å¾ˆå¥½çš„ï¼Œå› ä¸º
                  è¿™æ ·å¯ä»¥æ›´è½»æ¾åœ°è§£ææ¨¡å‹çš„è¾“å‡º
                  ä¸‹æ¸¸ï¼Œå¹¶äº†è§£ç”Ÿæˆåœæ­¢çš„åŸå› ã€‚
            run_manager: å…·æœ‰ LLM å›è°ƒçš„è¿è¡Œç®¡ç†å™¨ã€‚
        """
        # ç”¨å®é™…é€»è¾‘æ›¿æ¢æ­¤å¤„ä»¥ä»æ¶ˆæ¯åˆ—è¡¨ç”Ÿæˆå“åº”ã€‚
        last_message = messages[-1]
        tokens = last_message.content[: self.n]
        message = AIMessage(
            content=tokens,
            additional_kwargs={},  # ç”¨äºæ·»åŠ é™„åŠ è´Ÿè½½ï¼ˆä¾‹å¦‚ï¼Œå‡½æ•°è°ƒç”¨è¯·æ±‚ï¼‰
            response_metadata={  # ç”¨äºå“åº”å…ƒæ•°æ®
                "time_in_seconds": 3,
            },
        )
        ##
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        """æµå¼å¤„ç†æ¨¡å‹çš„è¾“å‡ºã€‚
        å¦‚æœæ¨¡å‹æ”¯æŒæµå¼å¤„ç†ï¼Œåˆ™åº”å®ç°æ­¤æ–¹æ³•ã€‚
        å¦‚æœæ¨¡å‹ä¸æ”¯æŒæµå¼å¤„ç†ï¼Œ
        åˆ™ä¸è¦å®ç°å®ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæµå¼å¤„ç†è¯·æ±‚å°†è‡ªåŠ¨å¤„ç†
        ç”± _generate æ–¹æ³•ã€‚
        Args:
            messages: ç”±æ¶ˆæ¯åˆ—è¡¨ç»„æˆçš„æç¤ºã€‚
            stop: æ¨¡å‹åº”åœæ­¢ç”Ÿæˆçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
                  å¦‚æœç”Ÿæˆç”±äºåœæ­¢ä»¤ç‰Œè€Œåœæ­¢ï¼Œåˆ™åœæ­¢ä»¤ç‰Œæœ¬èº«
                  åº”åŒ…å«åœ¨è¾“å‡ºä¸­ã€‚ç›®å‰å°šæœªå¼ºåˆ¶æ‰§è¡Œ
                  è·¨æ¨¡å‹ï¼Œä½†éµå¾ªè¿™ä¸€å®è·µæ˜¯å¾ˆå¥½çš„ï¼Œå› ä¸º
                  è¿™æ ·å¯ä»¥æ›´è½»æ¾åœ°è§£ææ¨¡å‹çš„è¾“å‡º
                  ä¸‹æ¸¸ï¼Œå¹¶äº†è§£ç”Ÿæˆåœæ­¢çš„åŸå› ã€‚
            run_manager: å…·æœ‰ LLM å›è°ƒçš„è¿è¡Œç®¡ç†å™¨ã€‚
        """
        last_message = messages[-1]
        tokens = last_message.content[: self.n]
        for token in tokens:
            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))
            if run_manager:
                # åœ¨è¾ƒæ–°ç‰ˆæœ¬çš„ LangChain ä¸­ï¼Œè¿™æ˜¯å¯é€‰çš„
                # on_llm_new_token å°†è‡ªåŠ¨è°ƒç”¨
                run_manager.on_llm_new_token(token, chunk=chunk)
            yield chunk
        # è®©æˆ‘ä»¬æ·»åŠ ä¸€äº›å…¶ä»–ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œå“åº”å…ƒæ•°æ®ï¼‰
        chunk = ChatGenerationChunk(
            message=AIMessageChunk(content="", response_metadata={"time_in_sec": 3})
        )
        if run_manager:
            # åœ¨è¾ƒæ–°ç‰ˆæœ¬çš„ LangChain ä¸­ï¼Œè¿™æ˜¯å¯é€‰çš„
            # on_llm_new_token å°†è‡ªåŠ¨è°ƒç”¨
            run_manager.on_llm_new_token(token, chunk=chunk)
        yield chunk
    @property
    def _llm_type(self) -> str:
        """è·å–æ­¤èŠå¤©æ¨¡å‹ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ç±»å‹ã€‚"""
        return "echoing-chat-model-advanced"
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›ä¸€ä¸ªæ ‡è¯†å‚æ•°çš„å­—å…¸ã€‚
        æ­¤ä¿¡æ¯ç”± LangChain å›è°ƒç³»ç»Ÿä½¿ç”¨ï¼Œè¯¥ç³»ç»Ÿ
        ç”¨äºè·Ÿè¸ªç›®çš„ï¼Œä½¿å…¶èƒ½å¤Ÿç›‘è§† LLMã€‚
        """
        return {
            # æ¨¡å‹åç§°å…è®¸ç”¨æˆ·åœ¨ LLM ç›‘æ§åº”ç”¨ç¨‹åºä¸­æŒ‡å®šè‡ªå®šä¹‰ä»¤ç‰Œè®¡æ•°è§„åˆ™
            # ï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangSmith ä¸­ï¼Œç”¨æˆ·å¯ä»¥ä¸ºå…¶æ¨¡å‹æä¾›æ¯ä¸ªä»¤ç‰Œçš„å®šä»·ï¼Œå¹¶ç›‘è§†
            # ç»™å®š LLM çš„æˆæœ¬ã€‚ï¼‰
            "model_name": self.model_name,
        }
```
### è®©æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹ ğŸ§ª
èŠå¤©æ¨¡å‹å°†å®ç° LangChain çš„æ ‡å‡† `Runnable` æ¥å£ï¼Œè®¸å¤š LangChain æŠ½è±¡æ”¯æŒè¿™ä¸€æ¥å£ï¼
```python
model = CustomChatModelAdvanced(n=3, model_name="my_custom_model")
model.invoke(
    [
        HumanMessage(content="hello!"),
        AIMessage(content="Hi there human!"),
        HumanMessage(content="Meow!"),
    ]
)
```
```output
AIMessage(content='Meo', response_metadata={'time_in_seconds': 3}, id='run-ddb42bd6-4fdd-4bd2-8be5-e11b67d3ac29-0')
```
```python
model.invoke("hello")
```
```output
AIMessage(content='hel', response_metadata={'time_in_seconds': 3}, id='run-4d3cc912-44aa-454b-977b-ca02be06c12e-0')
```
```python
model.batch(["hello", "goodbye"])
```
```output
[AIMessage(content='hel', response_metadata={'time_in_seconds': 3}, id='run-9620e228-1912-4582-8aa1-176813afec49-0'),
 AIMessage(content='goo', response_metadata={'time_in_seconds': 3}, id='run-1ce8cdf8-6f75-448e-82f7-1bb4a121df93-0')]
```
```python
for chunk in model.stream("cat"):
    print(chunk.content, end="|")
```
```output
c|a|t||
```
è¯·æŸ¥çœ‹æ¨¡å‹ä¸­ `_astream` çš„å®ç°ï¼å¦‚æœä¸å®ç°å®ƒï¼Œå°†ä¸ä¼šæœ‰è¾“å‡ºæµã€‚
```python
async for chunk in model.astream("cat"):
    print(chunk.content, end="|")
```
```output
c|a|t||
```
è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ astream äº‹ä»¶ APIï¼Œè¿™ä¹Ÿå°†æœ‰åŠ©äºåŒé‡æ£€æŸ¥æ‰€æœ‰å›è°ƒæ˜¯å¦å·²å®ç°ï¼
```python
async for event in model.astream_events("cat", version="v1"):
    print(event)
```
```output
{'event': 'on_chat_model_start', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'name': 'CustomChatModelAdvanced', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='c', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='a', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='t', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='', response_metadata={'time_in_sec': 3}, id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_end', 'name': 'CustomChatModelAdvanced', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', response_metadata={'time_in_sec': 3}, id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
``````output
/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.
  warn_beta(
```
## è´¡çŒ®
æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰èŠå¤©æ¨¡å‹é›†æˆçš„è´¡çŒ®ã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ£€æŸ¥åˆ—è¡¨ï¼Œå¯å¸®åŠ©ç¡®ä¿æ‚¨çš„è´¡çŒ®è¢«æ·»åŠ åˆ° LangChain ä¸­ï¼š
æ–‡æ¡£ï¼š
* æ¨¡å‹åŒ…å«æ‰€æœ‰åˆå§‹åŒ–å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå› ä¸ºè¿™äº›å°†æ˜¾ç¤ºåœ¨ [APIReference](https://api.python.langchain.com/en/stable/langchain_api_reference.html) ä¸­ã€‚
* å¦‚æœæ¨¡å‹ç”±æœåŠ¡æä¾›æ”¯æŒï¼Œåˆ™æ¨¡å‹çš„ç±»æ–‡æ¡£å­—ç¬¦ä¸²åŒ…å«æŒ‡å‘æ¨¡å‹ API çš„é“¾æ¥ã€‚
æµ‹è¯•ï¼š
* [ ] ä¸ºé‡å†™çš„æ–¹æ³•æ·»åŠ å•å…ƒæµ‹è¯•æˆ–é›†æˆæµ‹è¯•ã€‚å¦‚æœæ‚¨å·²é‡å†™ç›¸åº”çš„ä»£ç ï¼Œè¯·éªŒè¯ `invoke`ã€`ainvoke`ã€`batch`ã€`stream` æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
æµå¼å¤„ç†ï¼ˆå¦‚æœæ‚¨æ­£åœ¨å®ç°ï¼‰ï¼š
* [ ] å®ç° _stream æ–¹æ³•ä»¥ä½¿æµå¼å¤„ç†æ­£å¸¸å·¥ä½œ
åœæ­¢æ ‡è®°è¡Œä¸ºï¼š
* [ ] åº”éµå®ˆåœæ­¢æ ‡è®°
* [ ] åœæ­¢æ ‡è®°åº”åŒ…å«åœ¨å“åº”ä¸­
ç§˜å¯† API å¯†é’¥ï¼š
* [ ] å¦‚æœæ‚¨çš„æ¨¡å‹è¿æ¥åˆ° APIï¼Œåˆ™å¯èƒ½ä¼šå°† API å¯†é’¥ä½œä¸ºåˆå§‹åŒ–çš„ä¸€éƒ¨åˆ†ã€‚ä½¿ç”¨ Pydantic çš„ `SecretStr` ç±»å‹æ¥å¤„ç†ç§˜å¯†ï¼Œä»¥é˜²æ­¢åœ¨æ‰“å°æ¨¡å‹æ—¶æ„å¤–æ³„éœ²ã€‚
è¯†åˆ«å‚æ•°ï¼š
* [ ] åœ¨è¯†åˆ«å‚æ•°ä¸­åŒ…å« `model_name`
ä¼˜åŒ–ï¼š
è€ƒè™‘æä¾›æœ¬æœºå¼‚æ­¥æ”¯æŒä»¥å‡å°‘æ¨¡å‹çš„å¼€é”€ï¼
* [ ] æä¾› `_agenerate` çš„æœ¬æœºå¼‚æ­¥æ”¯æŒï¼ˆç”± `ainvoke` ä½¿ç”¨ï¼‰
* [ ] æä¾› `_astream` çš„æœ¬æœºå¼‚æ­¥æ”¯æŒï¼ˆç”± `astream` ä½¿ç”¨ï¼‰
## ä¸‹ä¸€æ­¥
æ‚¨ç°åœ¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åˆ›å»ºè‡ªå·±çš„å®šåˆ¶èŠå¤©æ¨¡å‹ã€‚
æ¥ä¸‹æ¥ï¼Œè¯·æŸ¥çœ‹æœ¬éƒ¨åˆ†ä¸­æœ‰å…³èŠå¤©æ¨¡å‹çš„å…¶ä»–æ“ä½œæŒ‡å—ï¼Œæ¯”å¦‚[å¦‚ä½•è®©æ¨¡å‹è¿”å›ç»“æ„åŒ–è¾“å‡º](/docs/how_to/structured_output)æˆ–[å¦‚ä½•è·Ÿè¸ªèŠå¤©æ¨¡å‹ä»¤ç‰Œä½¿ç”¨æƒ…å†µ](/docs/how_to/chat_token_usage_tracking)ã€‚