# åœ¨ Intel Xeon ä¸Šçš„ RAG ç¤ºä¾‹

è¯¥æ¨¡æ¿ä½¿ç”¨ Chroma å’Œæ–‡æœ¬ç”Ÿæˆæ¨ç†åœ¨ IntelÂ® XeonÂ® å¯æ‰©å±•å¤„ç†å™¨ä¸Šæ‰§è¡Œ RAGã€‚

IntelÂ® XeonÂ® å¯æ‰©å±•å¤„ç†å™¨å…·æœ‰å†…ç½®åŠ é€Ÿå™¨ï¼Œå¯å®ç°æ›´é«˜çš„æ¯æ ¸æ€§èƒ½å’Œæ— ä¸ä¼¦æ¯”çš„äººå·¥æ™ºèƒ½æ€§èƒ½ï¼Œé…å¤‡å…ˆè¿›çš„å®‰å…¨æŠ€æœ¯ï¼Œæ»¡è¶³æœ€é«˜éœ€æ±‚çš„å·¥ä½œè´Ÿè½½è¦æ±‚ï¼ŒåŒæ—¶æä¾›æœ€å¤§çš„äº‘é€‰æ‹©å’Œåº”ç”¨å¯ç§»æ¤æ€§ï¼Œè¯·æŸ¥çœ‹[IntelÂ® XeonÂ® å¯æ‰©å±•å¤„ç†å™¨](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html)ã€‚

## ç¯å¢ƒè®¾ç½®

è¦åœ¨ IntelÂ® XeonÂ® å¯æ‰©å±•å¤„ç†å™¨ä¸Šä½¿ç”¨[ğŸ¤— æ–‡æœ¬ç”Ÿæˆæ¨ç†](https://github.com/huggingface/text-generation-inference)ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

### åœ¨ Intel Xeon æœåŠ¡å™¨ä¸Šå¯åŠ¨æœ¬åœ°æœåŠ¡å™¨å®ä¾‹ï¼š

```bash
model=Intel/neural-chat-7b-v3-3
volume=$PWD/data # ä¸ Docker å®¹å™¨å…±äº«å·ï¼Œé¿å…æ¯æ¬¡è¿è¡Œéƒ½ä¸‹è½½æƒé‡
docker run --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4 --model-id $model
```

å¯¹äºåƒ `LLAMA-2` è¿™æ ·çš„é—¨æ§æ¨¡å‹ï¼Œæ‚¨éœ€è¦åœ¨ä¸Šè¿° docker è¿è¡Œå‘½ä»¤ä¸­ä¼ é€’ -e HUGGING_FACE_HUB_TOKEN=\<token\>ï¼Œå¹¶ä½¿ç”¨æœ‰æ•ˆçš„ Hugging Face Hub è¯»å–ä»¤ç‰Œã€‚

è¯·æŒ‰ç…§æ­¤é“¾æ¥[huggingface token](https://huggingface.co/docs/hub/security-tokens)è·å–è®¿é—®ä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨è¯¥ä»¤ç‰Œå¯¼å‡º `HUGGINGFACEHUB_API_TOKEN` ç¯å¢ƒå˜é‡ã€‚

```bash
export HUGGINGFACEHUB_API_TOKEN=<token> 
```

å‘é€è¯·æ±‚ä»¥æ£€æŸ¥ç«¯ç‚¹æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```bash
curl localhost:8080/generate -X POST -d '{"inputs":"Which NFL team won the Super Bowl in the 2010 season?","parameters":{"max_new_tokens":128, "do_sample": true}}'   -H 'Content-Type: application/json'
```

æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ[text-generation-inference](https://github.com/huggingface/text-generation-inference)ã€‚

## æ•°æ®å¡«å……

å¦‚æœæ‚¨æƒ³è¦ä½¿ç”¨ä¸€äº›ç¤ºä¾‹æ•°æ®å¡«å……æ•°æ®åº“ï¼Œå¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```shell
poetry install
poetry run python ingest.py
```

è¯¥è„šæœ¬ä¼šå¤„ç†å¹¶å°†æ¥è‡ª Nike `nke-10k-2023.pdf` çš„ Edgar 10k æ–‡ä»¶æ•°æ®éƒ¨åˆ†å­˜å‚¨åˆ° Chroma æ•°æ®åº“ä¸­ã€‚

## ä½¿ç”¨

è¦ä½¿ç”¨æ­¤è½¯ä»¶åŒ…ï¼Œæ‚¨é¦–å…ˆåº”å®‰è£… LangChain CLIï¼š

```shell
pip install -U langchain-cli
```

è¦åˆ›å»ºä¸€ä¸ªæ–°çš„ LangChain é¡¹ç›®å¹¶å°†å…¶å®‰è£…ä¸ºå”¯ä¸€è½¯ä»¶åŒ…ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œï¼š

```shell
langchain app new my-app --package intel-rag-xeon
```

å¦‚æœè¦å°†å…¶æ·»åŠ åˆ°ç°æœ‰é¡¹ç›®ä¸­ï¼Œåªéœ€è¿è¡Œï¼š

```shell
langchain app add intel-rag-xeon
```

å¹¶å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°æ‚¨çš„ `server.py` æ–‡ä»¶ä¸­ï¼š

```python
from intel_rag_xeon import chain as xeon_rag_chain
add_routes(app, xeon_rag_chain, path="/intel-rag-xeon")
```

ï¼ˆå¯é€‰ï¼‰ç°åœ¨è®©æˆ‘ä»¬é…ç½® LangSmithã€‚LangSmith å°†å¸®åŠ©æˆ‘ä»¬è·Ÿè¸ªã€ç›‘æ§å’Œè°ƒè¯• LangChain åº”ç”¨ç¨‹åºã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ³¨å†Œ LangSmith [here](https://smith.langchain.com/)ã€‚å¦‚æœæ‚¨æ²¡æœ‰è®¿é—®æƒé™ï¼Œå¯ä»¥è·³è¿‡æ­¤éƒ¨åˆ†

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>
export LANGCHAIN_PROJECT=<your-project>  # å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º "default"
```

å¦‚æœæ‚¨åœ¨æ­¤ç›®å½•ä¸­ï¼Œåˆ™å¯ä»¥ç›´æ¥å¯åŠ¨ LangServe å®ä¾‹ï¼š

```shell
langchain serve
```

è¿™å°†å¯åŠ¨ FastAPI åº”ç”¨ç¨‹åºï¼ŒæœåŠ¡å™¨åœ¨æœ¬åœ°è¿è¡Œï¼Œåœ°å€ä¸º

[http://localhost:8000](http://localhost:8000)

æˆ‘ä»¬å¯ä»¥åœ¨[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)çœ‹åˆ°æ‰€æœ‰æ¨¡æ¿ã€‚

æˆ‘ä»¬å¯ä»¥åœ¨[http://127.0.0.1:8000/intel-rag-xeon/playground](http://127.0.0.1:8000/intel-rag-xeon/playground)è®¿é—® playgroundã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç è®¿é—®æ¨¡æ¿ï¼š

```python
from langserve.client import RemoteRunnable
runnable = RemoteRunnable("http://localhost:8000/intel-rag-xeon")
```