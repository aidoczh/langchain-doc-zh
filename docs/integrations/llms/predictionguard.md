# é¢„æµ‹å®ˆå«

```python
%pip install --upgrade --quiet predictionguard langchain
```

```python
import os
from langchain.chains import LLMChain
from langchain_community.llms import PredictionGuard
from langchain_core.prompts import PromptTemplate
```

## åŸºæœ¬ LLM ä½¿ç”¨

```python
# å¯é€‰ï¼Œæ·»åŠ ä½ çš„ OpenAI API å¯†é’¥ã€‚è¿™æ˜¯å¯é€‰çš„ï¼Œå› ä¸º Prediction Guard å…è®¸ä½ è®¿é—®æ‰€æœ‰æœ€æ–°çš„å¼€æ”¾è®¿é—®æ¨¡å‹ï¼ˆè¯·å‚é˜… https://docs.predictionguard.comï¼‰
os.environ["OPENAI_API_KEY"] = "<ä½ çš„ OpenAI API å¯†é’¥>"
# ä½ çš„ Prediction Guard API å¯†é’¥ã€‚åœ¨ predictionguard.com è·å–
os.environ["PREDICTIONGUARD_TOKEN"] = "<ä½ çš„ Prediction Guard è®¿é—®ä»¤ç‰Œ>"
```

```python
pgllm = PredictionGuard(model="OpenAI-text-davinci-003")
```

```python
pgllm("å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯")
```

## æ§åˆ¶ LLM çš„è¾“å‡ºç»“æ„/ç±»å‹

```python
template = """æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚
ä¸Šä¸‹æ–‡ï¼šæ¯æ¡è¯„è®ºã€ç§ä¿¡å’Œç”µå­é‚®ä»¶å»ºè®®éƒ½å¼•é¢†æˆ‘ä»¬åšå‡ºè¿™ä¸ªä»¤äººå…´å¥‹çš„å…¬å‘Šï¼ğŸ‰ æˆ‘ä»¬æ­£å¼æ·»åŠ äº†ä¸¤ä¸ªæ–°çš„èœ¡çƒ›è®¢é˜…ç›’é€‰é¡¹ï¼ğŸ“¦
ç‹¬å®¶èœ¡çƒ›ç›’ - $80
æœˆåº¦èœ¡çƒ›ç›’ - $45ï¼ˆå…¨æ–°ï¼ï¼‰
æœ¬æœˆé¦™å‘³ç›’ - $28ï¼ˆå…¨æ–°ï¼ï¼‰
å‰å¾€æ•…äº‹äº†è§£æ¯ä¸ªç›’å­çš„æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ï¼ğŸ‘† å¥–åŠ±ï¼šä½¿ç”¨ä»£ç  50OFF å¯ä»¥èŠ‚çœé¦–ä¸ªç›’å­çš„ 50%ï¼ğŸ‰
é—®é¢˜ï¼š{query}
ç»“æœï¼š"""
prompt = PromptTemplate.from_template(template)
```

```python
# æ²¡æœ‰å¯¹ LLM çš„è¾“å‡ºè¿›è¡Œâ€œä¿æŠ¤â€æˆ–æ§åˆ¶ã€‚
pgllm(prompt.format(query="è¿™æ˜¯ä»€ä¹ˆæ ·çš„å¸–å­ï¼Ÿ"))
```

```python
# ä½¿ç”¨â€œä¿æŠ¤â€æˆ–æ§åˆ¶ LLM çš„è¾“å‡ºã€‚è¯·å‚é˜… Prediction Guard æ–‡æ¡£ï¼ˆhttps://docs.predictionguard.comï¼‰ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨æ•´æ•°ã€æµ®ç‚¹æ•°ã€å¸ƒå°”å€¼ã€JSON å’Œå…¶ä»–ç±»å‹å’Œç»“æ„æ¥æ§åˆ¶è¾“å‡ºã€‚
pgllm = PredictionGuard(
    model="OpenAI-text-davinci-003",
    output={
        "type": "categorical",
        "categories": ["äº§å“å…¬å‘Š", "é“æ­‰", "å…³ç³»å‹"],
    },
)
pgllm(prompt.format(query="è¿™æ˜¯ä»€ä¹ˆæ ·çš„å¸–å­ï¼Ÿ"))
```

## é“¾æ¥

```python
pgllm = PredictionGuard(model="OpenAI-text-davinci-003")
```

```python
template = """é—®é¢˜ï¼š{question}
å›ç­”ï¼šè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°æ€è€ƒã€‚"""
prompt = PromptTemplate.from_template(template)
llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)
question = "è´¾æ–¯æ±€Â·æ¯”ä¼¯å‡ºç”Ÿå¹´ä»½çš„è¶…çº§ç¢—å† å†›æ˜¯å“ªæ”¯ NFL çƒé˜Ÿï¼Ÿ"
llm_chain.predict(question=question)
```

```python
template = """å†™ä¸€é¦–å…³äº {subject} çš„ {adjective} è¯—ã€‚"""
prompt = PromptTemplate.from_template(template)
llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)
llm_chain.predict(adjective="æ‚²ä¼¤çš„", subject="é¸­å­")
```