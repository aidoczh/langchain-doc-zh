# å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰çš„LLMç±»
æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰çš„LLMåŒ…è£…å™¨ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±çš„LLMæˆ–ä¸LangChainæ”¯æŒçš„åŒ…è£…å™¨ä¸åŒçš„åŒ…è£…å™¨ã€‚
å°†æ‚¨çš„LLMä¸æ ‡å‡†çš„`LLM`æ¥å£åŒ…è£…åœ¨ä¸€èµ·ï¼Œå¯ä»¥è®©æ‚¨åœ¨ç°æœ‰çš„LangChainç¨‹åºä¸­ä½¿ç”¨æ‚¨çš„LLMï¼Œå¹¶ä¸”åªéœ€è¿›è¡Œæœ€å°‘çš„ä»£ç ä¿®æ”¹ï¼
ä½œä¸ºé¢å¤–çš„å¥½å¤„ï¼Œæ‚¨çš„LLMå°†è‡ªåŠ¨æˆä¸ºLangChainçš„`Runnable`ï¼Œå¹¶ä¸”å°†å—ç›Šäºä¸€äº›å¼€ç®±å³ç”¨çš„ä¼˜åŒ–ã€å¼‚æ­¥æ”¯æŒã€`astream_events` APIç­‰ã€‚
## å®ç°
è‡ªå®šä¹‰LLMåªéœ€è¦å®ç°ä¸¤ä¸ªå¿…éœ€çš„å†…å®¹ï¼š
| æ–¹æ³•           | æè¿°                                                         |
|----------------|--------------------------------------------------------------|
| `_call`        | æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²å’Œä¸€äº›å¯é€‰çš„åœç”¨è¯ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚è¢«`invoke`ä½¿ç”¨ã€‚ |
| `_llm_type`    | è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²çš„å±æ€§ï¼Œä»…ç”¨äºè®°å½•ç›®çš„ã€‚                       
å¯é€‰çš„å®ç°ï¼š
| æ–¹æ³•                | æè¿°                                                                                   |
|---------------------|----------------------------------------------------------------------------------------|
| `_identifying_params` | ç”¨äºå¸®åŠ©è¯†åˆ«æ¨¡å‹å¹¶æ‰“å°LLMï¼›åº”è¿”å›ä¸€ä¸ªå­—å…¸ã€‚è¿™æ˜¯ä¸€ä¸ª **@property**ã€‚                 |
| `_acall`             | æä¾›`_call`çš„å¼‚æ­¥æœ¬æœºå®ç°ï¼Œè¢«`ainvoke`ä½¿ç”¨ã€‚                                           |
| `_stream`            | é€ä¸ªä»¤ç‰Œæµå¼è¾“å‡ºçš„æ–¹æ³•ã€‚                                                               |
| `_astream`           | æä¾›`_stream`çš„å¼‚æ­¥æœ¬æœºå®ç°ï¼›åœ¨è¾ƒæ–°çš„LangChainç‰ˆæœ¬ä¸­ï¼Œé»˜è®¤ä¸º`_stream`ã€‚             |
è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„è‡ªå®šä¹‰LLMï¼Œå®ƒåªè¿”å›è¾“å…¥çš„å‰nä¸ªå­—ç¬¦ã€‚
```python
from typing import Any, Dict, Iterator, List, Mapping, Optional
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
class CustomLLM(LLM):
    """ä¸€ä¸ªè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ï¼Œå›æ˜¾è¾“å…¥çš„æœ€å`n`ä¸ªå­—ç¬¦ã€‚
    åœ¨ä¸ºLangChainè´¡çŒ®å®ç°æ—¶ï¼Œä»”ç»†è®°å½•æ¨¡å‹ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–å‚æ•°ï¼ŒåŒ…æ‹¬å¦‚ä½•åˆå§‹åŒ–æ¨¡å‹çš„ç¤ºä¾‹ï¼Œå¹¶åŒ…æ‹¬ä»»ä½•ç›¸å…³çš„é“¾æ¥åˆ°åº•å±‚æ¨¡å‹çš„æ–‡æ¡£æˆ–APIã€‚
    ç¤ºä¾‹ï¼š
        .. code-block:: python
            model = CustomChatModel(n=2)
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """
    n: int
    """è¦å›æ˜¾çš„è¾“å…¥çš„å­—ç¬¦æ•°ã€‚"""
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """åœ¨ç»™å®šè¾“å…¥ä¸Šè¿è¡ŒLLMã€‚
        é‡å†™æ­¤æ–¹æ³•ä»¥å®ç°LLMé€»è¾‘ã€‚
        å‚æ•°ï¼š
            promptï¼šè¦ç”Ÿæˆçš„æç¤ºã€‚
            stopï¼šåœ¨ç”Ÿæˆæ—¶è¦ä½¿ç”¨çš„åœç”¨è¯ã€‚æ¨¡å‹è¾“å‡ºåœ¨ä»»ä½•åœç”¨å­å­—ç¬¦ä¸²çš„ç¬¬ä¸€æ¬¡å‡ºç°æ—¶è¢«æˆªæ–­ã€‚
                å¦‚æœä¸æ”¯æŒåœç”¨è¯ï¼Œè¯·è€ƒè™‘å¼•å‘NotImplementedErrorã€‚
            run_managerï¼šè¿è¡Œçš„å›è°ƒç®¡ç†å™¨ã€‚
            **kwargsï¼šä»»æ„çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚è¿™äº›é€šå¸¸ä¼ é€’ç»™æ¨¡å‹æä¾›è€…APIè°ƒç”¨ã€‚
        è¿”å›ï¼š
            ä½œä¸ºå­—ç¬¦ä¸²çš„æ¨¡å‹è¾“å‡ºã€‚å®é™…å®Œæˆä¸åº”åŒ…æ‹¬æç¤ºã€‚
        """
        if stop is not None:
            raise ValueError("ä¸å…è®¸ä½¿ç”¨åœç”¨è¯å‚æ•°ã€‚")
        return prompt[: self.n]
    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """åœ¨ç»™å®šæç¤ºä¸Šæµå¼ä¼ è¾“LLMã€‚
        å­ç±»åº”è¯¥é‡å†™æ­¤æ–¹æ³•ä»¥æ”¯æŒæµå¼ä¼ è¾“ã€‚
        å¦‚æœæœªå®ç°ï¼Œè°ƒç”¨streamçš„é»˜è®¤è¡Œä¸ºå°†é€€å›åˆ°æ¨¡å‹çš„éæµå¼ç‰ˆæœ¬ï¼Œå¹¶å°†è¾“å‡ºä½œä¸ºå•ä¸ªå—è¿”å›ã€‚
        å‚æ•°ï¼š
            promptï¼šè¦ç”Ÿæˆçš„æç¤ºã€‚
            stopï¼šåœ¨ç”Ÿæˆæ—¶è¦ä½¿ç”¨çš„åœç”¨è¯ã€‚æ¨¡å‹è¾“å‡ºåœ¨ä»»ä½•è¿™äº›å­å­—ç¬¦ä¸²çš„ç¬¬ä¸€æ¬¡å‡ºç°æ—¶è¢«æˆªæ–­ã€‚
            run_managerï¼šè¿è¡Œçš„å›è°ƒç®¡ç†å™¨ã€‚
            **kwargsï¼šä»»æ„çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚è¿™äº›é€šå¸¸ä¼ é€’ç»™æ¨¡å‹æä¾›è€…APIè°ƒç”¨ã€‚
        è¿”å›ï¼š
            ä¸€ä¸ªGenerationChunksçš„è¿­ä»£å™¨ã€‚
        """
        for char in prompt[: self.n]:
            chunk = GenerationChunk(text=char)
            if run_manager:
                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
            yield chunk
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›ä¸€ä¸ªæ ‡è¯†å‚æ•°çš„å­—å…¸ã€‚"""
        return {
            # æ¨¡å‹åç§°å…è®¸ç”¨æˆ·åœ¨LLMç›‘æ§åº”ç”¨ç¨‹åºä¸­æŒ‡å®šè‡ªå®šä¹‰ä»¤ç‰Œè®¡æ•°è§„åˆ™ï¼ˆä¾‹å¦‚ï¼Œåœ¨LangSmithä¸­ï¼Œç”¨æˆ·å¯ä»¥ä¸ºå…¶æ¨¡å‹æä¾›æ¯ä¸ªä»¤ç‰Œçš„å®šä»·ï¼Œå¹¶ç›‘è§†ç»™å®šLLMçš„æˆæœ¬ã€‚ï¼‰
            "model_name": "CustomChatModel",
        }
    @property
    def _llm_type(self) -> str:
        """è·å–æ­¤èŠå¤©æ¨¡å‹ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹çš„ç±»å‹ã€‚ä»…ç”¨äºè®°å½•ç›®çš„ã€‚"""
        return "custom"
```
### è®©æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹ ğŸ§ª
è¿™ä¸ª LLM å°†å®ç° LangChain çš„æ ‡å‡† `Runnable` æ¥å£ï¼Œè®¸å¤š LangChain æŠ½è±¡éƒ½æ”¯æŒï¼
```python
llm = CustomLLM(n=5)
print(llm)
```
```output
CustomLLM
å‚æ•°: {'model_name': 'CustomChatModel'}
```
```python
llm.invoke("è¿™æ˜¯ä¸€ä¸ª foobar ä¸œè¥¿")
```
```output
'This '
```
```python
await llm.ainvoke("world")
```
```output
'world'
```
```python
llm.batch(["woof woof woof", "meow meow meow"])
```
```output
['woof ', 'meow ']
```
```python
await llm.abatch(["woof woof woof", "meow meow meow"])
```
```output
['woof ', 'meow ']
```
```python
async for token in llm.astream("hello"):
    print(token, end="|", flush=True)
```
```output
h|e|l|l|o|
```
è®©æˆ‘ä»¬ç¡®è®¤å®ƒä¸å…¶ä»– `LangChain` API å¾ˆå¥½åœ°é›†æˆã€‚
```python
from langchain_core.prompts import ChatPromptTemplate
```
```python
prompt = ChatPromptTemplate.from_messages(
    [("system", "you are a bot"), ("human", "{input}")]
)
```
```python
llm = CustomLLM(n=7)
chain = prompt | llm
```
```python
idx = 0
async for event in chain.astream_events({"input": "hello there!"}, version="v1"):
    print(event)
    idx += 1
    if idx > 7:
        # æˆªæ–­
        break
```
```output
{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}
{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\nHuman: hello there!']}}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}
```
## è´¡çŒ®
æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰èŠå¤©æ¨¡å‹é›†æˆçš„è´¡çŒ®ã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ£€æŸ¥åˆ—è¡¨ï¼Œä»¥ç¡®ä¿æ‚¨çš„è´¡çŒ®è¢«æ·»åŠ åˆ° LangChain ä¸­ï¼š
æ–‡æ¡£ï¼š
* æ¨¡å‹åŒ…å«æ‰€æœ‰åˆå§‹åŒ–å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå› ä¸ºè¿™äº›å°†æ˜¾ç¤ºåœ¨ [APIReference](https://api.python.langchain.com/en/stable/langchain_api_reference.html) ä¸­ã€‚
* å¦‚æœæ¨¡å‹ç”±æœåŠ¡æä¾›æ”¯æŒï¼Œåˆ™æ¨¡å‹çš„ç±»æ–‡æ¡£å­—ç¬¦ä¸²åŒ…å«æŒ‡å‘æ¨¡å‹ API çš„é“¾æ¥ã€‚
æµ‹è¯•ï¼š
* [ ] ä¸ºé‡å†™çš„æ–¹æ³•æ·»åŠ å•å…ƒæµ‹è¯•æˆ–é›†æˆæµ‹è¯•ã€‚å¦‚æœæ‚¨å·²ç»é‡å†™äº†ç›¸åº”çš„ä»£ç ï¼Œè¯·éªŒè¯ `invoke`ã€`ainvoke`ã€`batch`ã€`stream` æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
æµå¼å¤„ç†ï¼ˆå¦‚æœæ‚¨æ­£åœ¨å®ç°å®ƒï¼‰ï¼š
* [ ] ç¡®ä¿è°ƒç”¨ `on_llm_new_token` å›è°ƒ
* [ ] åœ¨äº§ç”Ÿå—ä¹‹å‰è°ƒç”¨ `on_llm_new_token`
åœæ­¢ä»¤ç‰Œè¡Œä¸ºï¼š
* [ ] åº”å°Šé‡åœæ­¢ä»¤ç‰Œ
* [ ] åœæ­¢ä»¤ç‰Œåº”åŒ…å«åœ¨å“åº”ä¸­
ç§˜å¯† API å¯†é’¥ï¼š
* [ ] å¦‚æœæ‚¨çš„æ¨¡å‹è¿æ¥åˆ° APIï¼Œåˆ™å¯èƒ½ä¼šæ¥å— API å¯†é’¥ä½œä¸ºå…¶åˆå§‹åŒ–çš„ä¸€éƒ¨åˆ†ã€‚ä½¿ç”¨ Pydantic çš„ `SecretStr` ç±»å‹æ¥å¤„ç†ç§˜å¯†ï¼Œè¿™æ ·å½“ç”¨æˆ·æ‰“å°æ¨¡å‹æ—¶ä¸ä¼šæ„å¤–æ‰“å°å‡ºç§˜å¯†ã€‚
