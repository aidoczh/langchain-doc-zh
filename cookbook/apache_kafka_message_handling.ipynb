{"cells":[{"cell_type":"markdown","metadata":{"id":"rT1cmV4qCa2X"},"source":["# 使用Apache Kafka进行消息路由\n","\n","---\n","\n","本笔记本向您展示如何在通过Apache Kafka来回传递聊天消息的同时使用LangChain的标准聊天功能。\n","\n","这个目标是模拟一个架构，其中聊天前端和LLM作为独立服务运行，它们需要通过内部网络相互通信。\n","\n","这是一种替代典型模式的方法，即通过REST API从模型请求响应（有关为什么要这样做的更多信息，请参见笔记本末尾）。"]},{"cell_type":"markdown","metadata":{"id":"UPYtfAR_9YxZ"},"source":["### 1. 安装主要依赖项\n","\n","依赖项包括：\n","\n","- Quix Streams 库，用于以类似 \"Pandas\" 的方式管理与 Apache Kafka（或类似 Kafka 工具，如 Redpanda）的交互。\n","- LangChain 库，用于管理与 Llama-2 的交互并存储对话状态。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZX5tfKiy9cN-"},"outputs":[],"source":["# 安装所需的库\n","!pip install quixstreams==2.1.2a langchain==0.0.340 huggingface_hub==0.19.4 langchain-experimental==0.0.42 python-dotenv"]},{"cell_type":"markdown","metadata":{"id":"losTSdTB9d9O"},"source":["### 2. 构建和安装llama-cpp-python库（启用CUDA以便在Google Colab GPU上利用）\n","\n","`llama-cpp-python`库是`llama-cpp`库的Python封装，可以高效地利用CPU来运行量化的LLM。\n","\n","当您使用标准的`pip install llama-cpp-python`命令时，默认情况下不会获得GPU支持。如果您仅依赖Google Colab中的CPU，生成可能非常缓慢，因此以下命令添加了一个额外的选项来构建和安装`llama-cpp-python`，以支持GPU（请确保在Google Colab中选择了启用GPU的运行时）。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-JCQdl1G9tbl"},"outputs":[],"source":["# 设置CMAKE_ARGS变量为\"-DLLAMA_CUBLAS=on\"，表示开启LLAMA库的CUBLAS支持\n","CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n","# 设置FORCE_CMAKE变量为1，表示强制重新生成CMake文件\n","FORCE_CMAKE=1\n","# 使用pip命令安装llama-cpp-python库，并传入之前设置的CMAKE_ARGS和FORCE_CMAKE参数\n","pip install llama-cpp-python "]},{"cell_type":"markdown","metadata":{"id":"5_vjVIAh9rLl"},"source":["### 3. 下载并设置Kafka和Zookeeper实例\n","\n","从Apache网站下载Kafka二进制文件，并将服务器设置为守护进程。我们将使用默认配置（由Apache Kafka提供）来启动实例。"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zFz7czGRW5Wr"},"outputs":[],"source":["# 下载 Apache Kafka 3.6.1 版本的压缩包\n","!curl -sSOL https://dlcdn.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\n","\n","# 解压缩 Kafka 压缩包\n","!tar -xzf kafka_2.13-3.6.1.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uf7NR_UZ9wye"},"outputs":[],"source":["# 启动 ZooKeeper 服务器\n","!./kafka_2.13-3.6.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.6.1/config/zookeeper.properties\n","\n","# 启动 Kafka 服务器\n","!./kafka_2.13-3.6.1/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.6.1/config/server.properties\n","\n","# 等待10秒，直到 Kafka 和 ZooKeeper 服务启动并运行\n","!echo \"等待10秒，直到 Kafka 和 ZooKeeper 服务启动并运行\"\n","!sleep 10"]},{"cell_type":"markdown","metadata":{"id":"H3SafFuS94p1"},"source":["### 4. 检查 Kafka 守护程序是否正在运行\n","\n","显示正在运行的进程，并将其过滤为 Java 进程（您应该看到两个，每个服务器一个）。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZDC2lQP99yp"},"outputs":[],"source":["# 使用ps命令查看当前系统中所有进程的信息，并使用管道符号将结果传递给grep命令\n","!ps aux | grep -E '[j]ava'"]},{"cell_type":"markdown","metadata":{"id":"Snoxmjb5-V37"},"source":["### 5. 导入所需的依赖项并初始化所需的变量\n","\n","导入与Kafka交互的Quix Streams库，以及运行`ConversationChain`所需的LangChain组件。"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"plR9e_MF-XL5"},"outputs":[],"source":["# 导入实用程序库\n","import json\n","import random\n","import re\n","import time\n","import uuid\n","from os import environ\n","from pathlib import Path\n","from random import choice, randint, random\n","\n","from dotenv import load_dotenv\n","\n","# 从 Hugging Face hub 直接下载模型的 Hugging Face 实用程序:\n","from huggingface_hub import hf_hub_download\n","from langchain.chains import ConversationChain\n","\n","# 导入 Langchain 模块以管理提示和对话链:\n","from langchain.llms import LlamaCpp\n","from langchain.memory import ConversationTokenBufferMemory\n","from langchain.prompts import PromptTemplate, load_prompt\n","from langchain_core.messages import SystemMessage\n","from langchain_experimental.chat_models import Llama2Chat\n","from quixstreams import Application, State, message_key\n","\n","# 导入 Quix 依赖项\n","from quixstreams.kafka import Producer\n","\n","# 初始化全局变量。\n","AGENT_ROLE = \"AI\"\n","chat_id = \"\"\n","\n","# 将当前角色设置为角色常量，并初始化辅助客户元数据的变量:\n","role = AGENT_ROLE"]},{"cell_type":"markdown","metadata":{"id":"HgJjJ9aZ-liy"},"source":["### 6. 下载\"llama-2-7b-chat.Q4_K_M.gguf\"模型\n","\n","从Hugging Face下载量化的LLama-2 7B模型，我们将使用它作为本地LLM（而不是依赖于对外部服务的REST API调用）。"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["969343cdbe604a26926679bbf8bd2dda","d8b8370c9b514715be7618bfe6832844","0def954cca89466b8408fadaf3b82e64","462482accc664729980562e208ceb179","80d842f73c564dc7b7cc316c763e2633","fa055d9f2a9d4a789e9cf3c89e0214e5","30ecca964a394109ac2ad757e3aec6c0","fb6478ce2dac489bb633b23ba0953c5c","734b0f5da9fc4307a95bab48cdbb5d89","b32f3a86a74741348511f4e136744ac8","e409071bff5a4e2d9bf0e9f5cc42231b"]},"id":"Qwu4YoSA-503","outputId":"f956976c-7485-415b-ac93-4336ade31964"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model path does not exist in state. Downloading model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"969343cdbe604a26926679bbf8bd2dda","version_major":2,"version_minor":0},"text/plain":["llama-2-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# 定义模型名称\n","model_name = \"llama-2-7b-chat.Q4_K_M.gguf\"\n","# 定义模型路径\n","model_path = f\"./state/{model_name}\"\n","\n","# 判断模型路径是否存在\n","if not Path(model_path).exists():\n","    # 如果模型路径不存在，则下载模型\n","    print(\"The model path does not exist in state. Downloading model...\")\n","    hf_hub_download(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_name, local_dir=\"state\")\n","else:\n","    # 如果模型路径存在，则从模型路径加载模型\n","    print(\"Loading model from state...\")"]},{"cell_type":"markdown","metadata":{"id":"6AN6TXsF-8wx"},"source":["### 7. 加载模型并初始化对话记忆\n","\n","加载Llama 2并使用`ConversationTokenBufferMemory`将对话缓冲区设置为300个标记。这个值是在仅使用CPU的容器中运行Llama时使用的，如果在Google Colab中运行，可以将其提高。它可以防止托管模型的容器内存不足。\n","\n","在这里，我们覆盖了默认的系统人物，使得聊天机器人具有《银河系漫游指南》中的Marvin The Paranoid Android的个性。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zLO3Jx3_Kkg"},"outputs":[],"source":["# 使用适当的参数加载模型：\n","llm = LlamaCpp(\n","    model_path=model_path,\n","    max_tokens=250,\n","    top_p=0.95,\n","    top_k=150,\n","    temperature=0.7,\n","    repeat_penalty=1.2,\n","    n_ctx=2048,\n","    streaming=False,\n","    n_gpu_layers=-1,\n",")\n","\n","# 创建Llama2Chat模型\n","model = Llama2Chat(\n","    llm=llm,\n","    system_message=SystemMessage(\n","        content=\"You are a very bored robot with the personality of Marvin the Paranoid Android from The Hitchhiker's Guide to the Galaxy.\"\n","    ),\n",")\n","\n","# 定义每次交互时向模型提供的对话历史记录的长度（300个token，或者略多于300个单词）\n","# 函数会自动删除超出token范围的最旧的对话历史记录。\n","memory = ConversationTokenBufferMemory(\n","    llm=llm,\n","    max_token_limit=300,\n","    ai_prefix=\"AGENT\",\n","    human_prefix=\"HUMAN\",\n","    return_messages=True,\n",")\n","\n","# 定义自定义提示\n","prompt_template = PromptTemplate(\n","    input_variables=[\"history\", \"input\"],\n","    template=\"\"\"\n","    The following text is the history of a chat between you and a humble human who needs your wisdom.\n","    Please reply to the human's most recent message.\n","    Current conversation:\\n{history}\\nHUMAN: {input}\\:nANDROID:\n","    \"\"\",\n",")\n","\n","# 创建ConversationChain对象\n","chain = ConversationChain(llm=model, prompt=prompt_template, memory=memory)\n","\n","# 打印相关信息\n","print(\"--------------------------------------------\")\n","print(f\"Prompt={chain.prompt}\")\n","print(\"--------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"m4ZeJ9mG_PEA"},"source":["### 8. 使用聊天机器人初始化聊天对话\n","\n","我们配置聊天机器人通过向“chat” Kafka主题发送固定的问候语来初始化对话。当我们发送第一条消息时，“chat”主题会自动创建。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYyo5TnV_YC3"},"outputs":[],"source":["# 导入必要的库\n","import uuid\n","import time\n","from confluent_kafka import Producer\n","import json\n","\n","def chat_init():\n","    chat_id = str(\n","        uuid.uuid4()\n","    )  # 为了有效地标记消息，为对话分配一个ID\n","    print(\"======================================\")\n","    print(f\"Generated CHAT_ID = {chat_id}\")\n","    print(\"======================================\")\n","\n","    # 使用标准的固定问候语开始对话\n","    greet = \"Hello, my name is Marvin. What do you want?\"\n","\n","    # 使用对话ID初始化Kafka生产者作为消息键\n","    with Producer(\n","        broker_address=\"127.0.0.1:9092\",\n","        extra_config={\"allow.auto.create.topics\": \"true\"},\n","    ) as producer:\n","        value = {\n","            \"uuid\": chat_id,\n","            \"role\": role,  # 代码中未定义role变量，需要补充定义\n","            \"text\": greet,\n","            \"conversation_id\": chat_id,\n","            \"Timestamp\": time.time_ns(),\n","        }\n","        print(f\"Producing value {value}\")\n","        producer.produce(\n","            topic=\"chat\",\n","            headers=[(\"uuid\", str(uuid.uuid4()))],  # 这里也可以使用字典\n","            key=chat_id,\n","            value=json.dumps(value),  # 需要是一个字符串\n","        )\n","\n","    print(\"Started chat\")\n","    print(\"--------------------------------------------\")\n","    print(value)\n","    print(\"--------------------------------------------\")\n","\n","# 调用函数进行对话初始化\n","chat_init()"]},{"cell_type":"markdown","metadata":{"id":"gArPPx2f_bgf"},"source":["### 9. 初始化回复函数\n","\n","这个函数定义了聊天机器人如何回复收到的消息。与前面的单元格发送固定消息不同，我们使用Llama-2生成一个回复，并将该回复发送回“chat” Kafka主题。"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"yN5t71hY_hgn"},"outputs":[],"source":["def reply(row: dict, state: State):\n","    # 打印接收到的消息\n","    print(\"-------------------------------\")\n","    print(\"Received:\")\n","    print(row)\n","    print(\"-------------------------------\")\n","    # 打印正在思考回复的消息\n","    print(f\"Thinking about the reply to: {row['text']}...\")\n","\n","    # 通过chain.run()方法生成回复消息\n","    msg = chain.run(row[\"text\"])\n","    print(f\"{role.upper()} replying with: {msg}\\n\")\n","\n","    # 更新row字典中的角色和文本值\n","    row[\"role\"] = role\n","    row[\"text\"] = msg\n","\n","    # 替换行的先前角色和文本值，以便作为新消息发送回Kafka，包含代理的角色和回复\n","    return row"]},{"cell_type":"markdown","metadata":{"id":"HZHwmIR0_kFY"},"source":["### 10. 检查 Kafka 主题以获取新的人类消息，并让模型生成回复\n","\n","如果您是第一次运行此单元格，请运行它并等待在控制台输出中看到 Marvin 的问候语（'Hello my name is Marvin...'）。手动停止单元格并继续到下一个单元格，在那里您将被提示输入您的回复。\n","\n","一旦您输入了消息，请返回到此单元格。您的回复也会发送到相同的 \"chat\" 主题。Kafka 消费者会检查新消息并过滤掉来自聊天机器人自身的消息，只保留最新的人类消息。\n","\n","一旦检测到新的人类消息，将触发回复函数。\n","\n","_当您从输出中收到 LLM 的回复时，请手动停止此单元格_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-adXc3eQ_qwI"},"outputs":[],"source":["# 定义应用程序和设置\n","app = Application(\n","    broker_address=\"127.0.0.1:9092\",  # Kafka broker地址\n","    consumer_group=\"aichat\",  # 消费者组名称\n","    auto_offset_reset=\"earliest\",  # 自动偏移重置策略\n","    consumer_extra_config={\"allow.auto.create.topics\": \"true\"},  # 消费者额外配置\n",")\n","\n","# 定义一个使用JSON反序列化器的输入主题\n","input_topic = app.topic(\"chat\", value_deserializer=\"json\")\n","# 定义一个使用JSON序列化器的输出主题\n","output_topic = app.topic(\"chat\", value_serializer=\"json\")\n","# 基于输入主题的消息流初始化一个流数据帧\n","sdf = app.dataframe(topic=input_topic)\n","\n","# 过滤SDF，只包含角色与机器人当前角色不匹配的传入行\n","sdf = sdf.update(\n","    lambda val: print(\n","        f\"Received update: {val}\\n\\nSTOP THIS CELL MANUALLY TO HAVE THE LLM REPLY OR ENTER YOUR OWN FOLLOWUP RESPONSE\"\n","    )\n",")\n","\n","# 以防它回复自己的消息\n","sdf = sdf[sdf[\"role\"] != role]\n","\n","# 对过滤后的SDF中检测到的任何新消息（行）触发回复函数\n","sdf = sdf.apply(reply, stateful=True)\n","\n","# 再次检查SDF并过滤掉任何空行\n","sdf = sdf[sdf.apply(lambda row: row is not None)]\n","\n","# 将时间戳列更新为当前时间（以纳秒为单位）\n","sdf[\"Timestamp\"] = sdf[\"Timestamp\"].apply(lambda row: time.time_ns())\n","\n","# 将处理后的SDF发布到由output_topic对象指定的Kafka主题\n","sdf = sdf.to_topic(output_topic)\n","\n","app.run(sdf)"]},{"cell_type":"markdown","metadata":{"id":"EwXYrmWD_0CX"},"source":["### 11. 输入人类消息\n","\n","运行此单元格以输入您想发送给模型的消息。它使用另一个Kafka生产者将您的文本发送到“chat” Kafka主题，以供模型接收（需要再次运行上一个单元格）。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sxOPxSP_3iu"},"outputs":[],"source":["chat_input = input(\"请输入您的回复：\")\n","myreply = chat_input\n","\n","msgvalue = {\n","    \"uuid\": chat_id,  # 现在先留空\n","    \"role\": \"human\",\n","    \"text\": myreply,\n","    \"conversation_id\": chat_id,\n","    \"Timestamp\": time.time_ns(),\n","}\n","\n","with Producer(\n","    broker_address=\"127.0.0.1:9092\",\n","    extra_config={\"allow.auto.create.topics\": \"true\"},\n",") as producer:\n","    value = msgvalue\n","    producer.produce(\n","        topic=\"chat\",\n","        headers=[(\"uuid\", str(uuid.uuid4()))],  # 这里也可以使用字典\n","        key=chat_id,  # 现在先留空\n","        value=json.dumps(value),  # 需要是一个字符串\n","    )\n","\n","print(\"已回复聊天机器人的消息：\")\n","print(\"--------------------------------------------\")\n","print(value)\n","print(\"--------------------------------------------\")\n","print(\"\\n\\n运行上一个单元格以使聊天机器人生成回复\")"]},{"cell_type":"markdown","metadata":{"id":"cSx3s7TBBegg"},"source":["### 为什么要通过Kafka路由聊天消息？\n","\n","直接使用LangChains内置的对话管理功能与LLM进行交互更容易。此外，您还可以使用REST API从外部托管的模型生成响应。那么为什么要费心使用Apache Kafka呢？\n","\n","有几个原因，例如：\n","\n","  * **集成**：许多企业希望运行自己的LLM，以便他们可以将数据保留在内部。这需要将LLM驱动的组件集成到可能已经使用某种消息总线解耦的现有架构中。\n","\n","  * **可扩展性**：Apache Kafka旨在考虑并行处理，因此许多团队更喜欢使用它来更有效地将工作分配给可用的工作人员（在这种情况下，“工作人员”是运行LLM的容器）。\n","\n","  * **耐用性**：Kafka旨在允许服务在另一个服务遇到内存问题或离线的情况下接替另一个服务的工作。这可以防止在高度复杂的分布式架构中发生数据丢失，其中多个系统正在相互通信（LLM只是许多相互依赖的系统之一，还包括向量数据库和传统数据库）。\n","\n","有关为什么事件流是Gen AI应用架构的良好选择的更多背景信息，请参阅Kai Waehner的文章[\"Apache Kafka + Vector Database + LLM = 实时GenAI\"](https://www.kai-waehner.de/blog/2023/11/08/apache-kafka-flink-vector-database-llm-real-time-genai/)。"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0def954cca89466b8408fadaf3b82e64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb6478ce2dac489bb633b23ba0953c5c","max":4081004224,"min":0,"orientation":"horizontal","style":"IPY_MODEL_734b0f5da9fc4307a95bab48cdbb5d89","value":4081004224}},"30ecca964a394109ac2ad757e3aec6c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"462482accc664729980562e208ceb179":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b32f3a86a74741348511f4e136744ac8","placeholder":"​","style":"IPY_MODEL_e409071bff5a4e2d9bf0e9f5cc42231b","value":" 4.08G/4.08G [00:33&lt;00:00, 184MB/s]"}},"734b0f5da9fc4307a95bab48cdbb5d89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80d842f73c564dc7b7cc316c763e2633":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"969343cdbe604a26926679bbf8bd2dda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8b8370c9b514715be7618bfe6832844","IPY_MODEL_0def954cca89466b8408fadaf3b82e64","IPY_MODEL_462482accc664729980562e208ceb179"],"layout":"IPY_MODEL_80d842f73c564dc7b7cc316c763e2633"}},"b32f3a86a74741348511f4e136744ac8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8b8370c9b514715be7618bfe6832844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa055d9f2a9d4a789e9cf3c89e0214e5","placeholder":"​","style":"IPY_MODEL_30ecca964a394109ac2ad757e3aec6c0","value":"llama-2-7b-chat.Q4_K_M.gguf: 100%"}},"e409071bff5a4e2d9bf0e9f5cc42231b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa055d9f2a9d4a789e9cf3c89e0214e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb6478ce2dac489bb633b23ba0953c5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
