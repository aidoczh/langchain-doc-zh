# å¦‚ä½•ä¸ºèŠå¤©æœºå™¨äººæ·»åŠ æ£€ç´¢åŠŸèƒ½
æ£€ç´¢æ˜¯èŠå¤©æœºå™¨äººå¸¸ç”¨çš„ä¸€ç§æŠ€æœ¯ï¼Œç”¨äºåœ¨èŠå¤©æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¹‹å¤–å¢åŠ æ•°æ®ä»¥å¢å¼ºå…¶å“åº”ã€‚æœ¬èŠ‚å°†ä»‹ç»å¦‚ä½•åœ¨èŠå¤©æœºå™¨äººçš„ä¸Šä¸‹æ–‡ä¸­å®ç°æ£€ç´¢ï¼Œä½†å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ£€ç´¢æ˜¯ä¸€ä¸ªéå¸¸å¾®å¦™å’Œæ·±å…¥çš„è¯é¢˜ - æˆ‘ä»¬é¼“åŠ±æ‚¨æ¢ç´¢[æ–‡æ¡£çš„å…¶ä»–éƒ¨åˆ†](/docs/how_to#qa-with-rag)ï¼Œè¿™äº›éƒ¨åˆ†ä¼šæ›´æ·±å…¥åœ°ä»‹ç»è¿™ä¸ªè¯é¢˜ï¼
## è®¾ç½®
æ‚¨éœ€è¦å®‰è£…ä¸€äº›è½¯ä»¶åŒ…ï¼Œå¹¶å°†æ‚¨çš„ OpenAI API å¯†é’¥è®¾ç½®ä¸ºåä¸º `OPENAI_API_KEY` çš„ç¯å¢ƒå˜é‡ï¼š
```python
%pip install -qU langchain langchain-openai langchain-chroma beautifulsoup4
# è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ–ä» .env æ–‡ä»¶ä¸­åŠ è½½ï¼š
import dotenv
dotenv.load_dotenv()
```
```output
è­¦å‘Šï¼šæ‚¨æ­£åœ¨ä½¿ç”¨ pip ç‰ˆæœ¬ 22.0.4ï¼›ç„¶è€Œï¼Œç‰ˆæœ¬ 23.3.2 å¯ç”¨ã€‚
æ‚¨åº”è¯¥è€ƒè™‘é€šè¿‡ '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' å‘½ä»¤è¿›è¡Œå‡çº§ã€‚
æ³¨æ„ï¼šæ‚¨å¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨å†…æ ¸ä»¥ä½¿ç”¨æ›´æ–°åçš„è½¯ä»¶åŒ…ã€‚
```
```output
True
```
è®©æˆ‘ä»¬ä¹Ÿè®¾ç½®ä¸€ä¸ªèŠå¤©æ¨¡å‹ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ä½¿ç”¨ã€‚
```python
from langchain_openai import ChatOpenAI
chat = ChatOpenAI(model="gpt-3.5-turbo-1106", temperature=0.2)
```
## åˆ›å»ºæ£€ç´¢å™¨
æˆ‘ä»¬å°†ä½¿ç”¨[LangSmithæ–‡æ¡£](https://docs.smith.langchain.com/overview)ä½œä¸ºæºææ–™ï¼Œå¹¶å°†å†…å®¹å­˜å‚¨åœ¨ä¸€ä¸ªå‘é‡å­˜å‚¨ä¸­ä»¥ä¾›ä»¥åæ£€ç´¢ã€‚è¯·æ³¨æ„ï¼Œæœ¬ç¤ºä¾‹å°†ç•¥è¿‡ä¸€äº›å…³äºè§£æå’Œå­˜å‚¨æ•°æ®æºçš„å…·ä½“ç»†èŠ‚ - æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](/docs/how_to#qa-with-rag)çœ‹åˆ°æ›´å¤šå…³äºåˆ›å»ºæ£€ç´¢ç³»ç»Ÿçš„æ·±å…¥æ–‡æ¡£ã€‚
è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ–‡æ¡£åŠ è½½å™¨ä»æ–‡æ¡£ä¸­æå–æ–‡æœ¬ï¼š
```python
from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
data = loader.load()
```
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å…¶åˆ†æˆè¾ƒå°çš„å—ï¼Œä»¥ä¾¿ LLM çš„ä¸Šä¸‹æ–‡çª—å£å¯ä»¥å¤„ç†ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ä¸€ä¸ªå‘é‡æ•°æ®åº“ä¸­ï¼š
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
all_splits = text_splitter.split_documents(data)
```
ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›å—åµŒå…¥å¹¶å­˜å‚¨åœ¨ä¸€ä¸ªå‘é‡æ•°æ®åº“ä¸­ï¼š
```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
```
æœ€åï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬åˆå§‹åŒ–çš„å‘é‡å­˜å‚¨ä¸­åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼š
```python
# k æ˜¯è¦æ£€ç´¢çš„å—æ•°
retriever = vectorstore.as_retriever(k=4)
docs = retriever.invoke("Can LangSmith help test my LLM applications?")
docs
```
```output
[Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})]
```
æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè°ƒç”¨ä¸Šé¢çš„æ£€ç´¢å™¨ä¼šå¯¼è‡´ LangSmith æ–‡æ¡£çš„æŸäº›éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººåœ¨å›ç­”é—®é¢˜æ—¶å¯ä»¥ä½¿ç”¨çš„æµ‹è¯•ä¿¡æ¯ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå¯ä»¥ä» LangSmith æ–‡æ¡£ä¸­è¿”å›ç›¸å…³æ•°æ®çš„æ£€ç´¢å™¨ï¼
## æ–‡æ¡£é“¾
ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå¯ä»¥è¿”å› LangChain æ–‡æ¡£çš„æ£€ç´¢å™¨ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¯ä»¥ä½¿ç”¨å®ƒä»¬ä½œä¸ºä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜çš„é“¾ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `create_stuff_documents_chain` è¾…åŠ©å‡½æ•°æ¥å°†æ‰€æœ‰è¾“å…¥æ–‡æ¡£â€œå¡«å……â€åˆ°æç¤ºä¸­ã€‚å®ƒè¿˜å°†å¤„ç†å°†æ–‡æ¡£æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²çš„å·¥ä½œã€‚
é™¤äº†èŠå¤©æ¨¡å‹ï¼Œè¯¥å‡½æ•°è¿˜æœŸæœ›æœ‰ä¸€ä¸ªå…·æœ‰ `context` å˜é‡çš„æç¤ºï¼Œä»¥åŠä¸€ä¸ªåä¸º `messages` çš„èŠå¤©å†å²æ¶ˆæ¯å ä½ç¬¦ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªé€‚å½“çš„æç¤ºå¹¶å°†å…¶ä¼ é€’å¦‚ä¸‹ï¼š
```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
SYSTEM_TEMPLATE = """
å›ç­”ç”¨æˆ·çš„é—®é¢˜åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œè¯·ä¸è¦å‡­ç©ºæé€ ï¼Œåªéœ€è¯´â€œæˆ‘ä¸çŸ¥é“â€ï¼š
<context>
{context}
</context>
"""
question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            SYSTEM_TEMPLATE,
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
document_chain = create_stuff_documents_chain(chat, question_answering_prompt)
```
æˆ‘ä»¬å¯ä»¥å•ç‹¬è°ƒç”¨è¿™ä¸ª `document_chain` æ¥å›ç­”é—®é¢˜ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢æ£€ç´¢åˆ°çš„æ–‡æ¡£å’ŒåŒæ ·çš„é—®é¢˜ `LangSmith å¦‚ä½•å¸®åŠ©æµ‹è¯•ï¼Ÿ`ï¼š
```python
from langchain_core.messages import HumanMessage
document_chain.invoke(
    {
        "context": docs,
        "messages": [
            HumanMessage(content="LangSmith èƒ½å¸®åŠ©æµ‹è¯•æˆ‘çš„ LLM åº”ç”¨å—ï¼Ÿ")
        ],
    }
)
```
```output
'æ˜¯çš„ï¼ŒLangSmith å¯ä»¥å¸®åŠ©æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„ LLM åº”ç”¨ã€‚å®ƒç®€åŒ–äº†åˆå§‹è®¾ç½®ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥ç›‘è§†åº”ç”¨ç¨‹åºï¼Œè®°å½•æ‰€æœ‰è·Ÿè¸ªï¼Œå¯è§†åŒ–å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶åœ¨å‡ºç°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œæ•…éšœæ’é™¤ã€‚'
```
çœ‹èµ·æ¥ä¸é”™ï¼ä¸ºäº†æ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸ä½¿ç”¨ä¸Šä¸‹æ–‡æ–‡æ¡£å¹¶æ¯”è¾ƒç»“æœï¼š
```python
document_chain.invoke(
    {
        "context": [],
        "messages": [
            HumanMessage(content="LangSmith èƒ½å¸®åŠ©æµ‹è¯•æˆ‘çš„ LLM åº”ç”¨å—ï¼Ÿ")
        ],
    }
)
```
```output
"æˆ‘ä¸æ¸…æ¥š LangSmith å¯¹æµ‹è¯• LLM åº”ç”¨çš„å…·ä½“èƒ½åŠ›ã€‚æœ€å¥½ç›´æ¥è”ç³» LangSmith è¯¢é—®ä»–ä»¬çš„æœåŠ¡ä»¥åŠä»–ä»¬å¦‚ä½•ååŠ©æµ‹è¯•æ‚¨çš„ LLM åº”ç”¨ã€‚"
```
æˆ‘ä»¬å¯ä»¥çœ‹åˆ° LLM æ²¡æœ‰è¿”å›ä»»ä½•ç»“æœã€‚
## æ£€ç´¢é“¾
è®©æˆ‘ä»¬å°†è¿™ä¸ªæ–‡æ¡£é“¾ä¸æ£€ç´¢å™¨ç»“åˆèµ·æ¥ã€‚ä»¥ä¸‹æ˜¯è¿™ç§æ–¹å¼çš„ä¸€ç§å¯èƒ½æ€§ï¼š
```python
from typing import Dict
from langchain_core.runnables import RunnablePassthrough
def parse_retriever_input(params: Dict):
    return params["messages"][-1].content
retrieval_chain = RunnablePassthrough.assign(
    context=parse_retriever_input | retriever,
).assign(
    answer=document_chain,
)
```
ç»™å®šä¸€ä¸ªè¾“å…¥æ¶ˆæ¯åˆ—è¡¨ï¼Œæˆ‘ä»¬æå–åˆ—è¡¨ä¸­æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™æ£€ç´¢å™¨ä»¥è·å–ä¸€äº›æ–‡æ¡£ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™æˆ‘ä»¬çš„æ–‡æ¡£é“¾ä»¥ç”Ÿæˆæœ€ç»ˆçš„å“åº”ã€‚
è°ƒç”¨è¿™ä¸ªé“¾ç»“åˆäº†ä¸Šé¢æ¦‚è¿°çš„ä¸¤ä¸ªæ­¥éª¤ï¼š
```python
retrieval_chain.invoke(
    {
        "messages": [
            HumanMessage(content="LangSmith èƒ½å¸®åŠ©æµ‹è¯•æˆ‘çš„ LLM åº”ç”¨å—ï¼Ÿ")
        ],
    }
)
```
```output
{'messages': [HumanMessage(content='LangSmith èƒ½å¸®åŠ©æµ‹è¯•æˆ‘çš„ LLM åº”ç”¨å—ï¼Ÿ')],
 'context': [Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})],
 'answer': 'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}
```
æ˜¯çš„ï¼ŒLangSmithå¯ä»¥å¸®åŠ©æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„LLMåº”ç”¨ç¨‹åºã€‚å®ƒç®€åŒ–äº†åˆå§‹è®¾ç½®ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥ç›‘æ§åº”ç”¨ç¨‹åºï¼Œè®°å½•æ‰€æœ‰è·Ÿè¸ªï¼Œå¯è§†åŒ–å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡æ•°æ®ï¼Œå¹¶åœ¨å‡ºç°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œæ•…éšœæ’é™¤ã€‚æ‚¨è¿˜å¯ä»¥ç‚¹å‡»â€œåœ¨Playgroundä¸­æ‰“å¼€â€æŒ‰é’®ï¼Œè®¿é—®æˆ‘ä»¬è‡ªå·±æ„å»ºçš„Playgroundï¼Œä»¥ä¾¿åœ¨æ£€æŸ¥LLMè°ƒç”¨æ—¶å°è¯•æ›´æ”¹å•è¯æˆ–çŸ­è¯­ï¼Œä»¥æŸ¥çœ‹å‘ç”Ÿäº†ä»€ä¹ˆæƒ…å†µã€‚[20]
è¿™æ˜¯å› ä¸ºæ£€ç´¢å™¨æ²¡æœ‰å†…åœ¨çš„çŠ¶æ€æ¦‚å¿µï¼Œå®ƒåªä¼šæå–ä¸ç»™å®šæŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°†æŸ¥è¯¢è½¬åŒ–ä¸ºä¸€ä¸ªç‹¬ç«‹çš„æŸ¥è¯¢ï¼Œä¸åŒ…å«ä»»ä½•å¤–éƒ¨å¼•ç”¨çš„LLMã€‚
ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š
```python
from langchain_core.messages import AIMessage, HumanMessage
query_transform_prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="messages"),
        (
            "user",
            "ç»™å®šä¸Šè¿°å¯¹è¯ï¼Œè¯·ç”Ÿæˆä¸€ä¸ªæœç´¢æŸ¥è¯¢ä»¥æŸ¥æ‰¾ä¸å¯¹è¯ç›¸å…³çš„ä¿¡æ¯ã€‚åªå›ç­”æŸ¥è¯¢ï¼Œä¸è¦å›ç­”å…¶ä»–å†…å®¹ã€‚",
        ),
    ]
)
query_transformation_chain = query_transform_prompt | chat
query_transformation_chain.invoke(
    {
        "messages": [
            HumanMessage(content="LangSmithèƒ½å¸®åŠ©æµ‹è¯•æˆ‘çš„LLMåº”ç”¨å—ï¼Ÿ"),
            AIMessage(
                content="æ˜¯çš„ï¼ŒLangSmithå¯ä»¥å¸®åŠ©æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„LLMåº”ç”¨ã€‚å®ƒå…è®¸æ‚¨å¿«é€Ÿç¼–è¾‘ç¤ºä¾‹å¹¶å°†å…¶æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥æ‰©å±•è¯„ä¼°é›†çš„è¡¨é¢ç§¯ï¼Œæˆ–è€…å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æé«˜è´¨é‡æˆ–é™ä½æˆæœ¬ã€‚æ­¤å¤–ï¼ŒLangSmithè¿˜å¯ä»¥ç”¨äºç›‘è§†åº”ç”¨ç¨‹åºï¼Œè®°å½•æ‰€æœ‰è·Ÿè¸ªï¼Œå¯è§†åŒ–å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶åœ¨å‡ºç°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œæ•…éšœæ’é™¤ã€‚"
            ),
            HumanMessage(content="å‘Šè¯‰æˆ‘æ›´å¤šï¼"),
        ],
    }
)
```
```output
AIMessage(content='"LangSmith LLMåº”ç”¨ç¨‹åºæµ‹è¯•å’Œè¯„ä¼°"')
```
å¤ªæ£’äº†ï¼è¿™ä¸ªè½¬æ¢åçš„æŸ¥è¯¢å°†æ£€ç´¢ä¸LLMåº”ç”¨ç¨‹åºæµ‹è¯•ç›¸å…³çš„ä¸Šä¸‹æ–‡æ–‡æ¡£ã€‚
è®©æˆ‘ä»¬å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„æ£€ç´¢é“¾ä¸­ã€‚æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„æ£€ç´¢å™¨åŒ…è£…å¦‚ä¸‹ï¼š
```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch
query_transforming_retriever_chain = RunnableBranch(
    (
        lambda x: len(x.get("messages", [])) == 1,
        # å¦‚æœåªæœ‰ä¸€æ¡æ¶ˆæ¯ï¼Œæˆ‘ä»¬åªéœ€å°†è¯¥æ¶ˆæ¯çš„å†…å®¹ä¼ é€’ç»™æ£€ç´¢å™¨
        (lambda x: x["messages"][-1].content) | retriever,
    ),
    # å¦‚æœæœ‰å¤šæ¡æ¶ˆæ¯ï¼Œæˆ‘ä»¬å°†è¾“å…¥ä¼ é€’ç»™LLMé“¾ä»¥è½¬æ¢æŸ¥è¯¢ï¼Œç„¶åä¼ é€’ç»™æ£€ç´¢å™¨
    query_transform_prompt | chat | StrOutputParser() | retriever,
).with_config(run_name="chat_retriever_chain")
```
ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæŸ¥è¯¢è½¬æ¢é“¾æ¥ä½¿æˆ‘ä»¬çš„æ£€ç´¢é“¾æ›´èƒ½å¤Ÿå¤„ç†è¿™æ ·çš„åç»­é—®é¢˜ï¼š
```python
SYSTEM_TEMPLATE = """
```
å¯ä»¥ï¼ŒLangSmithå¯ä»¥å¸®åŠ©æµ‹è¯•å’Œè¯„ä¼°LLMï¼ˆè¯­è¨€æ¨¡å‹ï¼‰åº”ç”¨ç¨‹åºã€‚å®ƒç®€åŒ–äº†åˆå§‹è®¾ç½®ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥ç›‘è§†åº”ç”¨ç¨‹åºã€è®°å½•æ‰€æœ‰è·Ÿè¸ªã€å¯è§†åŒ–å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡æ•°æ®ï¼Œå¹¶åœ¨å‡ºç°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œæ•…éšœæ’é™¤ã€‚[20]
```markdown
{
  "messages": [
    HumanMessage(content="Can LangSmith help test my LLM applications?"),
    AIMessage(content="Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."),
    HumanMessage(content="Tell me more!")
  ],
  "context": [
    Document(page_content="LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith", metadata={"description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.", "language": "en", "source": "https://docs.smith.langchain.com/overview", "title": "LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith"}),
    Document(page_content="You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be", metadata={"description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.", "language": "en", "source": "https://docs.smith.langchain.com/overview", "title": "LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith"}),
    Document(page_content="Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain", metadata={"description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.", "language": "en", "source": "https://docs.smith.langchain.com/overview", "title": "LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith"}),
    Document(page_content="LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like", metadata={"description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.", "language": "en", "source": "https://docs.smith.langchain.com/overview", "title": "LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith"})
  ]
}
```
LangSmithç®€åŒ–äº†æ„å»ºå¯é LLMåº”ç”¨ç¨‹åºçš„åˆå§‹è®¾ç½®ï¼Œä½†å®ƒæ‰¿è®¤ä»ç„¶éœ€è¦åŠªåŠ›æé«˜æç¤ºã€é“¾æ¡å’Œä»£ç†çš„æ€§èƒ½ï¼Œä½¿å®ƒä»¬è¾¾åˆ°è¶³å¤Ÿå¯é çš„æ°´å¹³ï¼Œå¯ä»¥ç”¨äºç”Ÿäº§ã€‚å®ƒè¿˜æä¾›äº†æ‰‹åŠ¨å®¡æŸ¥å’Œæ³¨é‡Šè¿è¡Œçš„èƒ½åŠ›ï¼Œé€šè¿‡æ³¨é‡Šé˜Ÿåˆ—ï¼Œå…è®¸æ‚¨æ ¹æ®æ¨¡å‹ç±»å‹æˆ–è‡ªåŠ¨è¯„ä¼°åˆ†æ•°ç­‰æ ‡å‡†é€‰æ‹©è¿è¡Œè¿›è¡Œäººå·¥å®¡æŸ¥ã€‚è¿™ä¸ªåŠŸèƒ½ç‰¹åˆ«é€‚ç”¨äºè¯„ä¼°è‡ªåŠ¨è¯„ä¼°å™¨éš¾ä»¥å¤„ç†çš„ä¸»è§‚ç‰¹æ€§ã€‚
æ‚¨å¯ä»¥æŸ¥çœ‹[è¿™ä¸ªLangSmithè·Ÿè¸ª](https://smith.langchain.com/public/bb329a3b-e92a-4063-ad78-43f720fbb5a2/r)æ¥äº†è§£å†…éƒ¨æŸ¥è¯¢è½¬æ¢æ­¥éª¤ã€‚
## æµå¼å¤„ç†
å› ä¸ºè¿™ä¸ªé“¾æ¡æ˜¯ç”¨LCELæ„å»ºçš„ï¼Œæ‰€ä»¥æ‚¨å¯ä»¥ä½¿ç”¨ç†Ÿæ‚‰çš„æ–¹æ³•ï¼Œæ¯”å¦‚`.stream()`ï¼š
```python
stream = conversational_retrieval_chain.stream(
    {
        "messages": [
            HumanMessage(content="LangSmithèƒ½å¸®åŠ©æµ‹è¯•æˆ‘çš„LLMåº”ç”¨ç¨‹åºå—ï¼Ÿ"),
            AIMessage(
                content="æ˜¯çš„ï¼ŒLangSmithå¯ä»¥å¸®åŠ©æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„LLMåº”ç”¨ç¨‹åºã€‚å®ƒå…è®¸æ‚¨å¿«é€Ÿç¼–è¾‘ç¤ºä¾‹å¹¶å°†å…¶æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥æ‰©å±•è¯„ä¼°é›†çš„è¦†ç›–èŒƒå›´ï¼Œæˆ–è€…å¾®è°ƒæ¨¡å‹ä»¥æé«˜è´¨é‡æˆ–é™ä½æˆæœ¬ã€‚æ­¤å¤–ï¼ŒLangSmithè¿˜å¯ä»¥ç”¨äºç›‘è§†æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œè®°å½•æ‰€æœ‰è·Ÿè¸ªï¼Œå¯è§†åŒ–å»¶è¿Ÿå’Œæ ‡è®°ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶åœ¨å‡ºç°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œæ•…éšœæ’é™¤ã€‚"
            ),
            HumanMessage(content="å‘Šè¯‰æˆ‘æ›´å¤šï¼"),
        ],
    }
)
for chunk in stream:
    print(chunk)
```
```output
{'messages': [HumanMessage(content='LangSmithèƒ½å¸®åŠ©æµ‹è¯•æˆ‘çš„LLMåº”ç”¨ç¨‹åºå—ï¼Ÿ'), AIMessage(content='æ˜¯çš„ï¼ŒLangSmithå¯ä»¥å¸®åŠ©æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„LLMåº”ç”¨ç¨‹åºã€‚å®ƒå…è®¸æ‚¨å¿«é€Ÿç¼–è¾‘ç¤ºä¾‹å¹¶å°†å…¶æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥æ‰©å±•è¯„ä¼°é›†çš„è¦†ç›–èŒƒå›´ï¼Œæˆ–è€…å¾®è°ƒæ¨¡å‹ä»¥æé«˜è´¨é‡æˆ–é™ä½æˆæœ¬ã€‚æ­¤å¤–ï¼ŒLangSmithè¿˜å¯ä»¥ç”¨äºç›‘è§†æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œè®°å½•æ‰€æœ‰è·Ÿè¸ªï¼Œå¯è§†åŒ–å»¶è¿Ÿå’Œæ ‡è®°ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶åœ¨å‡ºç°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œæ•…éšœæ’é™¤ã€‚'), HumanMessage(content='å‘Šè¯‰æˆ‘æ›´å¤šï¼')]}
```
LangSmith æ¦‚è¿°å’Œç”¨æˆ·æŒ‡å— | ğŸ¦œï¸ğŸ› ï¸ LangSmith
æ„å»ºå¯é çš„LLMåº”ç”¨ç¨‹åºå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚LangChainç®€åŒ–äº†åˆå§‹è®¾ç½®ï¼Œä½†ä»éœ€è¦è¿›è¡Œä¸€äº›å·¥ä½œï¼Œä»¥æé«˜æç¤ºã€é“¾æ¡å’Œä»£ç†çš„æ€§èƒ½æ°´å¹³ï¼Œä½¿å®ƒä»¬è¶³å¤Ÿå¯é ï¼Œå¯ä»¥ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚[20]
æ‚¨è¿˜å¯ä»¥å¿«é€Ÿç¼–è¾‘ç¤ºä¾‹å¹¶å°†å…¶æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥æ‰©å±•è¯„ä¼°é›†çš„è¦†ç›–èŒƒå›´ï¼Œæˆ–å¾®è°ƒæ¨¡å‹ä»¥æé«˜è´¨é‡æˆ–é™ä½æˆæœ¬ã€‚ç›‘æ§å®Œæˆæ‰€æœ‰è¿™äº›æ­¥éª¤åï¼Œæ‚¨çš„åº”ç”¨ç¨‹åºå¯èƒ½ç»ˆäºå‡†å¤‡æŠ•å…¥ç”Ÿäº§ã€‚LangSmithè¿˜å¯ä»¥ç”¨äºç›‘è§†æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œæ–¹å¼ä¸æ‚¨ç”¨äºè°ƒè¯•çš„æ–¹å¼éå¸¸ç›¸ä¼¼ã€‚æ‚¨å¯ä»¥è®°å½•æ‰€æœ‰è·Ÿè¸ªã€å¯è§†åŒ–å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡æ•°æ®ï¼Œå¹¶åœ¨é—®é¢˜å‡ºç°æ—¶å¯¹ç‰¹å®šé—®é¢˜è¿›è¡Œæ•…éšœæ’é™¤ã€‚æ¯æ¬¡è¿è¡Œä¹Ÿå¯ä»¥ã€‚
è·³è½¬è‡³ä¸»è¦å†…å®¹ğŸ¦œï¸ğŸ› ï¸ LangSmith æ–‡æ¡£Python æ–‡æ¡£JS/TS æ–‡æ¡£æœç´¢è½¬è‡³åº”ç”¨ç¨‹åºLangSmithæ¦‚è¿°è¿½è¸ªæµ‹è¯•ä¸è¯„ä¼°ç»„ç»‡ä¸­å¿ƒLangSmithé£Ÿè°±æ¦‚è¿°åœ¨è¿™ä¸ªé¡µé¢ä¸ŠLangSmith æ¦‚è¿°å’Œç”¨æˆ·æŒ‡å—æ„å»ºå¯é çš„LLMåº”ç”¨ç¨‹åºå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚LangChainç®€åŒ–äº†åˆå§‹è®¾ç½®ï¼Œä½†ä»éœ€è¦è¿›è¡Œä¸€äº›å·¥ä½œï¼Œä»¥æé«˜æç¤ºã€é“¾æ¡å’Œä»£ç†çš„æ€§èƒ½æ°´å¹³ï¼Œä½¿å®ƒä»¬è¶³å¤Ÿå¯é ï¼Œå¯ä»¥ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚åœ¨è¿‡å»çš„ä¸¤ä¸ªæœˆé‡Œï¼Œæˆ‘ä»¬åœ¨LangChain
LangSmithä½¿æ‰‹åŠ¨å®¡æŸ¥å’Œæ³¨é‡Šè¿è¡Œå˜å¾—ç®€å•é€šè¿‡æ³¨é‡Šé˜Ÿåˆ—ã€‚è¿™äº›é˜Ÿåˆ—å…è®¸æ‚¨æ ¹æ®æ¨¡å‹ç±»å‹æˆ–è‡ªåŠ¨è¯„åˆ†ç­‰æ ‡å‡†é€‰æ‹©ä»»ä½•è¿è¡Œï¼Œå¹¶å°†å…¶æ’é˜Ÿç­‰å¾…äººå·¥å®¡æŸ¥ã€‚ä½œä¸ºå®¡é˜…è€…ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿæµè§ˆè¿è¡Œï¼ŒæŸ¥çœ‹è¾“å…¥ã€è¾“å‡ºå’Œä»»ä½•ç°æœ‰æ ‡ç­¾ï¼Œç„¶åæ·»åŠ æ‚¨è‡ªå·±çš„åé¦ˆã€‚æˆ‘ä»¬ç»å¸¸å‡ºäºå‡ ä¸ªåŸå› ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ï¼šè¯„ä¼°è‡ªåŠ¨è¯„ä¼°å™¨éš¾ä»¥å¤„ç†çš„ä¸»è§‚ç‰¹è´¨ï¼Œå¦‚
è¿™ç§æ–¹æ³•å¯¹è¯„ä¼°è‡ªåŠ¨è¯„ä¼°å™¨éš¾ä»¥å¤„ç†çš„ä¸»è§‚ç‰¹è´¨å°¤å…¶æœ‰ç”¨ã€‚
## è¿›ä¸€æ­¥é˜…è¯»
æœ¬æŒ‡å—åªæ˜¯å¯¹æ£€ç´¢æŠ€æœ¯çš„æµ…æ˜¾ä»‹ç»ã€‚è¦äº†è§£æ›´å¤šæœ‰å…³ä¸åŒæ‘„å–ã€å‡†å¤‡å’Œæ£€ç´¢æœ€ç›¸å…³æ•°æ®çš„æ–¹æ³•ï¼Œè¯·æŸ¥é˜…ç›¸å…³çš„æ“ä½œæŒ‡å—[è¿™é‡Œ](/docs/how_to#document-loaders)ã€‚