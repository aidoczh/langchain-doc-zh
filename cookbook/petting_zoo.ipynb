{"cells":[{"cell_type":"markdown","id":"4b089493","metadata":{},"source":["# 多智能体模拟环境：宠物园\n","\n","在这个例子中，我们展示了如何使用模拟环境定义多智能体的模拟。就像我们之前使用Gymnasium定义单智能体的例子一样，我们创建了一个智能体-环境循环，其中环境是外部定义的。主要区别在于我们现在实现了多智能体的交互循环。我们将使用Petting Zoo库，它是Gymnasium的多智能体对应库。"]},{"cell_type":"markdown","id":"10091333","metadata":{},"source":["## 安装 `pettingzoo` 和其他依赖项"]},{"cell_type":"code","execution_count":1,"id":"0a3fde66","metadata":{},"outputs":[],"source":["# 安装所需的库\n","!pip install pettingzoo pygame rlcard"]},{"cell_type":"markdown","id":"5fbe130c","metadata":{},"source":["## 导入模块"]},{"cell_type":"code","execution_count":2,"id":"42cd2e5d","metadata":{},"outputs":[],"source":["# 导入必要的库\n","import collections\n","import inspect\n","\n","import tenacity\n","from langchain.output_parsers import RegexParser\n","from langchain.schema import (\n","    HumanMessage,  # 导入HumanMessage类\n","    SystemMessage,  # 导入SystemMessage类\n",")\n","from langchain_openai import ChatOpenAI  # 导入ChatOpenAI类"]},{"cell_type":"markdown","id":"e222e811","metadata":{},"source":["## `GymnasiumAgent`\n","在这里，我们复制了与[我们的Gymnasium示例](https://python.langchain.com/en/latest/use_cases/agent_simulations/gymnasium.html)中定义的相同的`GymnasiumAgent`。如果经过多次重试后仍然无法采取有效的动作，它将随机选择一个动作。"]},{"cell_type":"code","execution_count":3,"id":"72df0b59","metadata":{},"outputs":[],"source":["class GymnasiumAgent:\n","    @classmethod\n","    def get_docs(cls, env):\n","        return env.unwrapped.__doc__  # 获取环境的文档字符串\n","\n","    def __init__(self, model, env):\n","        self.model = model  # 模型\n","        self.env = env  # 环境\n","        self.docs = self.get_docs(env)  # 获取环境的文档字符串\n","\n","        self.instructions = \"\"\"\n","你的目标是最大化你的回报，即你所获得的奖励的总和。\n","我会给你一个观察值、奖励、终止标志、截断标志和迄今为止的回报，格式如下：\n","\n","观察值: <observation>\n","奖励: <reward>\n","终止标志: <termination>\n","截断标志: <truncation>\n","回报: <sum_of_rewards>\n","\n","你需要用一个动作来回应，格式如下：\n","\n","动作: <action>\n","\n","你需要将 <action> 替换为你的实际动作。\n","除此之外，不要做任何其他操作，只需返回动作。\n","\"\"\"\n","        self.action_parser = RegexParser(\n","            regex=r\"Action: (.*)\", output_keys=[\"action\"], default_output_key=\"action\"\n","        )  # 动作解析器\n","\n","        self.message_history = []  # 消息历史记录\n","        self.ret = 0  # 回报\n","\n","    def random_action(self):\n","        action = self.env.action_space.sample()  # 随机选择一个动作\n","        return action\n","\n","    def reset(self):\n","        self.message_history = [\n","            SystemMessage(content=self.docs),  # 添加环境的文档字符串到消息历史记录\n","            SystemMessage(content=self.instructions),  # 添加指令到消息历史记录\n","        ]\n","\n","    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n","        self.ret += rew  # 更新回报\n","\n","        obs_message = f\"\"\"\n","观察值: {obs}\n","奖励: {rew}\n","终止标志: {term}\n","截断标志: {trunc}\n","回报: {self.ret}\n","        \"\"\"\n","        self.message_history.append(HumanMessage(content=obs_message))  # 添加观察消息到消息历史记录\n","        return obs_message\n","\n","    def _act(self):\n","        act_message = self.model.invoke(self.message_history)  # 使用模型进行动作选择\n","        self.message_history.append(act_message)  # 添加动作消息到消息历史记录\n","        action = int(self.action_parser.parse(act_message.content)[\"action\"])  # 解析动作消息中的动作\n","        return action\n","\n","    def act(self):\n","        try:\n","            for attempt in tenacity.Retrying(\n","                stop=tenacity.stop_after_attempt(2),  # 最多尝试2次\n","                wait=tenacity.wait_none(),  # 重试之间没有等待时间\n","                retry=tenacity.retry_if_exception_type(ValueError),  # 如果出现 ValueError 异常则重试\n","                before_sleep=lambda retry_state: print(\n","                    f\"ValueError occurred: {retry_state.outcome.exception()}, retrying...\"\n","                ),  # 打印错误信息并重试\n","            ):\n","                with attempt:\n","                    action = self._act()  # 执行动作选择\n","        except tenacity.RetryError:\n","            action = self.random_action()  # 如果重试失败，则随机选择一个动作\n","        return action  # 返回动作"]},{"cell_type":"markdown","id":"df51e302","metadata":{},"source":["## 主循环"]},{"cell_type":"code","execution_count":4,"id":"0f07d7cf","metadata":{},"outputs":[],"source":["def main(agents, env):\n","    # 重置环境\n","    env.reset()\n","\n","    # 对每个智能体进行重置\n","    for name, agent in agents.items():\n","        agent.reset()\n","\n","    # 对环境中的每个智能体进行迭代\n","    for agent_name in env.agent_iter():\n","        # 获取环境的最新状态\n","        observation, reward, termination, truncation, info = env.last()\n","        # 让智能体观察环境并返回观察信息\n","        obs_message = agents[agent_name].observe(\n","            observation, reward, termination, truncation, info\n","        )\n","        print(obs_message)\n","        # 如果环境终止或被截断，则不执行动作\n","        if termination or truncation:\n","            action = None\n","        else:\n","            # 让智能体执行动作\n","            action = agents[agent_name].act()\n","        print(f\"Action: {action}\")\n","        # 在环境中执行动作\n","        env.step(action)\n","    # 关闭环境\n","    env.close()"]},{"cell_type":"markdown","id":"b4b0e921","metadata":{},"source":["## `PettingZooAgent`\n","\n","`PettingZooAgent`扩展了`GymnasiumAgent`以适应多智能体环境。主要的区别包括：\n","- `PettingZooAgent`接受一个`name`参数，用于在多个智能体中进行标识\n","- `get_docs`函数的实现方式不同，因为`PettingZoo`仓库的结构与`Gymnasium`仓库的结构不同。"]},{"cell_type":"code","execution_count":5,"id":"f132c92a","metadata":{},"outputs":[],"source":["# 定义一个PettingZooAgent类，继承自GymnasiumAgent类\n","class PettingZooAgent(GymnasiumAgent):\n","    # 定义一个类方法get_docs，用于获取环境的文档信息\n","    @classmethod\n","    def get_docs(cls, env):\n","        return inspect.getmodule(env.unwrapped).__doc__\n","\n","    # 定义初始化方法，接受name、model和env三个参数\n","    def __init__(self, name, model, env):\n","        # 调用父类的初始化方法\n","        super().__init__(model, env)\n","        # 设置实例变量name\n","        self.name = name\n","\n","    # 定义random_action方法，用于生成随机动作\n","    def random_action(self):\n","        # 从环境的动作空间中随机采样一个动作\n","        action = self.env.action_space(self.name).sample()\n","        return action"]},{"cell_type":"markdown","id":"a27f8a5d","metadata":{},"source":["## 石头，剪刀，布\n","现在我们可以使用`PettingZooAgent`来运行一个多智能体石头，剪刀，布游戏的模拟。"]},{"cell_type":"code","execution_count":6,"id":"bd1256c0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Observation: 3\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 1\n","\n","Observation: 3\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 1\n","\n","Observation: 1\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 2\n","\n","Observation: 1\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 1\n","\n","Observation: 1\n","Reward: 1\n","Termination: False\n","Truncation: False\n","Return: 1\n","        \n","Action: 0\n","\n","Observation: 2\n","Reward: -1\n","Termination: False\n","Truncation: False\n","Return: -1\n","        \n","Action: 0\n","\n","Observation: 0\n","Reward: 0\n","Termination: False\n","Truncation: True\n","Return: 1\n","        \n","Action: None\n","\n","Observation: 0\n","Reward: 0\n","Termination: False\n","Truncation: True\n","Return: -1\n","        \n","Action: None\n"]}],"source":["from pettingzoo.classic import rps_v2  # 导入rps_v2环境\n","\n","# 创建一个rps_v2环境，最大周期为3，渲染模式为\"human\"\n","env = rps_v2.env(max_cycles=3, render_mode=\"human\")\n","\n","# 创建一个代理字典，代理名称为环境中可能的代理名称，值为PettingZooAgent对象，使用ChatOpenAI模型，温度为1\n","agents = {\n","    name: PettingZooAgent(name=name, model=ChatOpenAI(temperature=1), env=env)\n","    for name in env.possible_agents\n","}\n","\n","# 调用main函数，传入代理字典和环境对象\n","main(agents, env)"]},{"cell_type":"markdown","id":"fbcee258","metadata":{},"source":["## `ActionMaskAgent`\n","\n","一些 `PettingZoo` 环境提供了一个 `action_mask` 来告诉代理程序哪些动作是有效的。`ActionMaskAgent` 是 `PettingZooAgent` 的子类，使用来自 `action_mask` 的信息来选择动作。"]},{"cell_type":"code","execution_count":7,"id":"bd33250a","metadata":{},"outputs":[],"source":["# 定义一个名为ActionMaskAgent的类，继承自PettingZooAgent类\n","class ActionMaskAgent(PettingZooAgent):\n","    # 初始化方法，接受name、model和env三个参数\n","    def __init__(self, name, model, env):\n","        # 调用父类的初始化方法\n","        super().__init__(name, model, env)\n","        # 创建一个长度为1的双向队列作为观察缓冲区\n","        self.obs_buffer = collections.deque(maxlen=1)\n","\n","    # 定义一个随机动作的方法\n","    def random_action(self):\n","        # 获取观察缓冲区中的最后一个观察\n","        obs = self.obs_buffer[-1]\n","        # 从环境的动作空间中随机采样一个动作，根据观察中的“action_mask”进行采样\n","        action = self.env.action_space(self.name).sample(obs[\"action_mask\"])\n","        return action\n","\n","    # 定义重置方法\n","    def reset(self):\n","        # 初始化消息历史，包括系统消息和指令消息\n","        self.message_history = [\n","            SystemMessage(content=self.docs),\n","            SystemMessage(content=self.instructions),\n","        ]\n","\n","    # 定义观察方法，接受obs、rew、term、trunc和info五个参数\n","    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n","        # 将观察添加到观察缓冲区中\n","        self.obs_buffer.append(obs)\n","        # 调用父类的观察方法\n","        return super().observe(obs, rew, term, trunc, info)\n","\n","    # 定义私有的动作方法\n","    def _act(self):\n","        # 有效动作指令\n","        valid_action_instruction = \"Generate a valid action given by the indices of the `action_mask` that are not 0, according to the action formatting rules.\"\n","        # 将有效动作指令添加到消息历史中\n","        self.message_history.append(HumanMessage(content=valid_action_instruction))\n","        # 调用父类的动作方法\n","        return super()._act()"]},{"cell_type":"markdown","id":"2e76d22c","metadata":{},"source":["## 井字游戏\n","这是一个使用`ActionMaskAgent`的井字游戏示例。"]},{"cell_type":"code","execution_count":8,"id":"9e902cfd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Observation: {'observation': array([[[0, 0],\n","        [0, 0],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 0\n","     |     |     \n","  X  |  -  |  -  \n","_____|_____|_____\n","     |     |     \n","  -  |  -  |  -  \n","_____|_____|_____\n","     |     |     \n","  -  |  -  |  -  \n","     |     |     \n","\n","Observation: {'observation': array([[[0, 1],\n","        [0, 0],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 1\n","     |     |     \n","  X  |  -  |  -  \n","_____|_____|_____\n","     |     |     \n","  O  |  -  |  -  \n","_____|_____|_____\n","     |     |     \n","  -  |  -  |  -  \n","     |     |     \n","\n","Observation: {'observation': array([[[1, 0],\n","        [0, 1],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 2\n","     |     |     \n","  X  |  -  |  -  \n","_____|_____|_____\n","     |     |     \n","  O  |  -  |  -  \n","_____|_____|_____\n","     |     |     \n","  X  |  -  |  -  \n","     |     |     \n","\n","Observation: {'observation': array([[[0, 1],\n","        [1, 0],\n","        [0, 1]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 1, 1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 3\n","     |     |     \n","  X  |  O  |  -  \n","_____|_____|_____\n","     |     |     \n","  O  |  -  |  -  \n","_____|_____|_____\n","     |     |     \n","  X  |  -  |  -  \n","     |     |     \n","\n","Observation: {'observation': array([[[1, 0],\n","        [0, 1],\n","        [1, 0]],\n","\n","       [[0, 1],\n","        [0, 0],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 4\n","     |     |     \n","  X  |  O  |  -  \n","_____|_____|_____\n","     |     |     \n","  O  |  X  |  -  \n","_____|_____|_____\n","     |     |     \n","  X  |  -  |  -  \n","     |     |     \n","\n","Observation: {'observation': array([[[0, 1],\n","        [1, 0],\n","        [0, 1]],\n","\n","       [[1, 0],\n","        [0, 1],\n","        [0, 0]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 5\n","     |     |     \n","  X  |  O  |  -  \n","_____|_____|_____\n","     |     |     \n","  O  |  X  |  -  \n","_____|_____|_____\n","     |     |     \n","  X  |  O  |  -  \n","     |     |     \n","\n","Observation: {'observation': array([[[1, 0],\n","        [0, 1],\n","        [1, 0]],\n","\n","       [[0, 1],\n","        [1, 0],\n","        [0, 1]],\n","\n","       [[0, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 6\n","     |     |     \n","  X  |  O  |  X  \n","_____|_____|_____\n","     |     |     \n","  O  |  X  |  -  \n","_____|_____|_____\n","     |     |     \n","  X  |  O  |  -  \n","     |     |     \n","\n","Observation: {'observation': array([[[0, 1],\n","        [1, 0],\n","        [0, 1]],\n","\n","       [[1, 0],\n","        [0, 1],\n","        [1, 0]],\n","\n","       [[0, 1],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)}\n","Reward: -1\n","Termination: True\n","Truncation: False\n","Return: -1\n","        \n","Action: None\n","\n","Observation: {'observation': array([[[1, 0],\n","        [0, 1],\n","        [1, 0]],\n","\n","       [[0, 1],\n","        [1, 0],\n","        [0, 1]],\n","\n","       [[1, 0],\n","        [0, 0],\n","        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)}\n","Reward: 1\n","Termination: True\n","Truncation: False\n","Return: 1\n","        \n","Action: None\n"]}],"source":["from pettingzoo.classic import tictactoe_v3\n","\n","env = tictactoe_v3.env(render_mode=\"human\")\n","agents = {\n","    name: ActionMaskAgent(name=name, model=ChatOpenAI(temperature=0.2), env=env)\n","    for name in env.possible_agents\n","}\n","main(agents, env)"]},{"cell_type":"markdown","id":"8728ac2a","metadata":{},"source":["## 德州扑克无限制版\n","这是一个使用`ActionMaskAgent`的德州扑克无限制版游戏的示例。"]},{"cell_type":"code","execution_count":9,"id":"e350c62b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n","       0., 0., 2.], dtype=float32), 'action_mask': array([1, 1, 0, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 1\n","\n","Observation: {'observation': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","       0., 0., 2.], dtype=float32), 'action_mask': array([1, 1, 0, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 1\n","\n","Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 1., 2.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 1\n","\n","Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 2., 2.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 0\n","\n","Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n","       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 2., 2.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 2\n","\n","Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n","       0., 2., 6.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 2\n","\n","Observation: {'observation': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n","       0., 2., 8.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 3\n","\n","Observation: {'observation': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n","        0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n","        1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","        6., 20.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 4\n","\n","Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,\n","         0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   8., 100.],\n","      dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n","Reward: 0\n","Termination: False\n","Truncation: False\n","Return: 0\n","        \n","Action: 4\n","[WARNING]: Illegal move made, game terminating with current player losing. \n","obs['action_mask'] contains a mask of all legal moves that can be chosen.\n","\n","Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,\n","         0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   8., 100.],\n","      dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n","Reward: -1.0\n","Termination: True\n","Truncation: True\n","Return: -1.0\n","        \n","Action: None\n","\n","Observation: {'observation': array([  0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,\n","         0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,  20., 100.],\n","      dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n","Reward: 0\n","Termination: True\n","Truncation: True\n","Return: 0\n","        \n","Action: None\n","\n","Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n","         1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 100., 100.],\n","      dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n","Reward: 0\n","Termination: True\n","Truncation: True\n","Return: 0\n","        \n","Action: None\n","\n","Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,\n","         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2., 100.],\n","      dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n","Reward: 0\n","Termination: True\n","Truncation: True\n","Return: 0\n","        \n","Action: None\n"]}],"source":["from pettingzoo.classic import texas_holdem_no_limit_v6\n","\n","env = texas_holdem_no_limit_v6.env(num_players=4, render_mode=\"human\")\n","agents = {\n","    name: ActionMaskAgent(name=name, model=ChatOpenAI(temperature=0.2), env=env)\n","    for name in env.possible_agents\n","}\n","main(agents, env)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}
