# OpenLLM

[ğŸ¦¾ OpenLLM](https://github.com/bentoml/OpenLLM) æ˜¯ä¸€ä¸ªç”¨äºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ“ä½œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼€æ”¾å¹³å°ã€‚å®ƒä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿè½»æ¾åœ°è¿è¡Œä»»ä½•å¼€æºLLMsçš„æ¨ç†ï¼Œéƒ¨ç½²åˆ°äº‘ç«¯æˆ–æœ¬åœ°ï¼Œå¹¶æ„å»ºå¼ºå¤§çš„äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºã€‚

## å®‰è£…

é€šè¿‡ [PyPI](https://pypi.org/project/openllm/) å®‰è£… `openllm`

```python
%pip install --upgrade --quiet  openllm
```

## åœ¨æœ¬åœ°å¯åŠ¨ OpenLLM æœåŠ¡å™¨

è¦å¯åŠ¨ä¸€ä¸ªLLMæœåŠ¡å™¨ï¼Œè¯·ä½¿ç”¨ `openllm start` å‘½ä»¤ã€‚ä¾‹å¦‚ï¼Œè¦å¯åŠ¨ä¸€ä¸ªdolly-v2æœåŠ¡å™¨ï¼Œè¯·ä»ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
openllm start dolly-v2
```

## åŒ…è£…å™¨

```python
from langchain_community.llms import OpenLLM
server_url = "http://localhost:3000"  # å¦‚æœåœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œè¯·æ›¿æ¢ä¸ºè¿œç¨‹ä¸»æœº
llm = OpenLLM(server_url=server_url)
```

### å¯é€‰ï¼šæœ¬åœ°LLMæ¨ç†

æ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©ä»å½“å‰è¿›ç¨‹ä¸­æœ¬åœ°åˆå§‹åŒ–ç”±OpenLLMç®¡ç†çš„LLMã€‚è¿™å¯¹äºå¼€å‘ç›®çš„å¾ˆæœ‰ç”¨ï¼Œå¹¶å…è®¸å¼€å‘äººå‘˜å¿«é€Ÿå°è¯•ä¸åŒç±»å‹çš„LLMsã€‚

å½“å°†LLMåº”ç”¨ç¨‹åºç§»è‡³ç”Ÿäº§ç¯å¢ƒæ—¶ï¼Œæˆ‘ä»¬å»ºè®®å•ç‹¬éƒ¨ç½²OpenLLMæœåŠ¡å™¨ï¼Œå¹¶é€šè¿‡ä¸Šé¢æ¼”ç¤ºçš„ `server_url` é€‰é¡¹è¿›è¡Œè®¿é—®ã€‚

è¦é€šè¿‡LangChainåŒ…è£…å™¨æœ¬åœ°åŠ è½½LLMï¼š

```python
from langchain_community.llms import OpenLLM
llm = OpenLLM(
    model_name="dolly-v2",
    model_id="databricks/dolly-v2-3b",
    temperature=0.94,
    repetition_penalty=1.2,
)
```

### ä¸LLMChainé›†æˆ

```python
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
template = "ä¸€ä¸ªåˆ¶é€ {product}çš„å…¬å¸åº”è¯¥å–ä»€ä¹ˆå¥½åå­—ï¼Ÿ"
prompt = PromptTemplate.from_template(template)
llm_chain = LLMChain(prompt=prompt, llm=llm)
generated = llm_chain.run(product="æœºæ¢°é”®ç›˜")
print(generated)
```

```output
iLkb
```